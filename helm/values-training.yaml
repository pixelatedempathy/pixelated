# Training-specific values for Pixelated Empathy Helm chart

# Global settings
global:
  environment: training

# Training service configuration
training:
  enabled: true
  namespace: pixelated-training
  replicaCount: 1
  
  image:
    repository: ghcr.io/pixelated/training-service
    tag: "latest"
    pullPolicy: IfNotPresent
  
  # Resources for H100 GPU training
  resources:
    requests:
      cpu: "4"
      memory: "16Gi"
      nvidia.com/gpu: 1
    limits:
      cpu: "16"
      memory: "64Gi"
      nvidia.com/gpu: 1
  
  # Node selection for GPU nodes
  nodeSelector:
    accelerator: nvidia-tesla-h100
    node-type: gpu
  
  # Tolerations for GPU nodes
  tolerations:
  - key: nvidia.com/gpu
    operator: Exists
    effect: NoSchedule
  
  # Affinity for optimal GPU scheduling
  affinity:
    nodeAffinity:
      requiredDuringSchedulingIgnoredDuringExecution:
        nodeSelectorTerms:
        - matchExpressions:
          - key: accelerator
            operator: In
            values: ["nvidia-tesla-h100", "nvidia-a100"]
  
  # Security contexts for HIPAA compliance
  podSecurityContext:
    runAsNonRoot: true
    runAsUser: 1001
    runAsGroup: 1001
    fsGroup: 1001
    seccompProfile:
      type: RuntimeDefault
    supplementalGroups: [1001]
  
  securityContext:
    allowPrivilegeEscalation: false
    readOnlyRootFilesystem: true
    runAsNonRoot: true
    runAsUser: 1001
    runAsGroup: 1001
    capabilities:
      drop:
      - ALL
    seccompProfile:
      type: RuntimeDefault
  
  # Environment variables
  env:
  - name: CUDA_VISIBLE_DEVICES
    value: "0"
  - name: PYTORCH_CUDA_ALLOC_CONF
    value: "max_split_size_mb:512"
  - name: LIGHTNING_CLOUD_PROJECT_ID
    valueFrom:
      secretKeyRef:
        name: training-secrets
        key: lightning-project-id
  - name: WANDB_API_KEY
    valueFrom:
      secretKeyRef:
        name: training-secrets
        key: wandb-api-key
  - name: HF_TOKEN
    valueFrom:
      secretKeyRef:
        name: training-secrets
        key: huggingface-token
  
  # Service configuration
  service:
    type: ClusterIP
    port: 80
    targetPort: 8003
  
  # Health checks
  livenessProbe:
    httpGet:
      path: /health
      port: 8003
    initialDelaySeconds: 120
    periodSeconds: 60
    timeoutSeconds: 30
    failureThreshold: 3
  
  readinessProbe:
    httpGet:
      path: /ready
      port: 8003
    initialDelaySeconds: 60
    periodSeconds: 30
    timeoutSeconds: 15
    failureThreshold: 5
  
  # Volume mounts
  volumeMounts:
  - name: training-data
    mountPath: /app/data
    readOnly: true
  - name: model-output
    mountPath: /app/models
  - name: checkpoints
    mountPath: /app/checkpoints
  - name: shared-memory
    mountPath: /dev/shm
  - name: cache
    mountPath: /app/cache
  
  # Volumes
  volumes:
  - name: training-data
    persistentVolumeClaim:
      claimName: training-data-pvc
  - name: model-output
    persistentVolumeClaim:
      claimName: model-output-pvc
  - name: checkpoints
    persistentVolumeClaim:
      claimName: checkpoints-pvc
  - name: shared-memory
    emptyDir:
      medium: Memory
      sizeLimit: 8Gi
  - name: cache
    emptyDir:
      sizeLimit: 10Gi

# Enhanced monitoring for training
monitoring:
  enabled: true
  
  prometheus:
    enabled: true
    serviceMonitor:
      enabled: true
      interval: 30s
      scrapeTimeout: 10s
      labels:
        app: training-service
    
    rules:
      enabled: true
      groups:
      - name: training.rules
        rules:
        - alert: TrainingServiceDown
          expr: up{job="training-service"} == 0
          for: 5m
          labels:
            severity: critical
          annotations:
            summary: "Training service is down"
            description: "Training service has been down for more than 5 minutes"
        
        - alert: GPUMemoryHigh
          expr: nvidia_gpu_memory_used_bytes / nvidia_gpu_memory_total_bytes > 0.9
          for: 10m
          labels:
            severity: warning
          annotations:
            summary: "GPU memory usage is high"
            description: "GPU memory usage is above 90% for more than 10 minutes"
        
        - alert: TrainingStalled
          expr: increase(training_batches_processed_total[30m]) == 0
          for: 30m
          labels:
            severity: critical
          annotations:
            summary: "Training appears to be stalled"
            description: "No training batches processed in the last 30 minutes"

# Storage configuration
storage:
  trainingData:
    enabled: true
    accessMode: ReadOnlyMany
    size: 500Gi
    storageClass: ssd-retain
  
  modelOutput:
    enabled: true
    accessMode: ReadWriteOnce
    size: 200Gi
    storageClass: ssd-retain
  
  checkpoints:
    enabled: true
    accessMode: ReadWriteOnce
    size: 100Gi
    storageClass: ssd-retain

# Network policies for security
networkPolicy:
  enabled: true
  ingress:
    enabled: true
    allowedNamespaces:
    - pixelated-prod
    - monitoring
  egress:
    enabled: true
    allowDNS: true
    allowHTTPS: true
    allowedPorts:
    - 5432  # PostgreSQL
    - 6379  # Redis