--- ai/training_ready/scripts/compile_final_dataset.py.corrupted	2026-01-30 22:57:35.130285971 -0500
+++ ai/training_ready/scripts/compile_final_dataset.py	2026-01-30 23:00:51.211412421 -0500
@@ -1,6 +1,23 @@
 #!/usr/bin/env python3
 """
-Compile Final Training Dataset - Creates manifest + compiled ChatML JSONL export
+Compile Final Training Dataset - v2 (Refactored & Optimized)
+
+Architecture:
+- Collects conversations from 14 data families (S3 + local generated)
+- Deduplicates using semantic similarity + content hashing
+- Validates using 8-gate quality checks
+- Creates train/val/test splits with leakage prevention
+- Shards dataset for efficient distributed training
+- Uploads to S3 with manifest metadata
+
+Flow:
+  1. Load checkpoint (resume-friendly)
+  2. Collect all conversations from all families
+  3. Deduplicate (semantic + hash-based)
+  4. Assign splits (train/val/test)
+  5. Check holdout family leakage
+  6. Create shards
+  7. Upload to S3
 """
 
 import argparse
@@ -9,18 +26,14 @@
 import json
 import logging
 import random
-import shutil
 import sys
 import time
 from collections import defaultdict
-from collections.abc import Callable
 from dataclasses import dataclass, field
 from datetime import datetime, timezone
 from pathlib import Path
 from typing import Any
 
-# Rich TUI removed - using logging instead
-
 from ai.training_ready.scripts.enhanced_deduplication import (
     ConversationEntry,
     EnhancedDeduplicator,
@@ -28,16 +41,28 @@
 )
 from ai.training_ready.utils.s3_dataset_loader import S3DatasetLoader
 
+# ============================================================================
+# Configuration & Logging
+# ============================================================================
+
 logger = logging.getLogger(__name__)
 logging.basicConfig(
-    level=logging.INFO, format="%(asctime)s - %(name)s - %(levelname)s - %(message)s"
+    level=logging.INFO,
+    format="%(asctime)s - %(levelname)s - %(message)s",
+    handlers=[
+        logging.StreamHandler(),
+        logging.FileHandler("compile_dataset.log"),
+    ],
 )
 
 
+# ============================================================================
+# Data Classes
+# ============================================================================
+
 @dataclass
 class DatasetShard:
     """Represents a sharded dataset file"""
-
     shard_id: str
     s3_path: str
     size_bytes: int
@@ -49,1344 +74,477 @@
 @dataclass
 class SplitInfo:
     """Information about a dataset split"""
-
     conversations: int
     shards: list[DatasetShard]
     total_tokens_approx: int = 0
 
 
 @dataclass
-class ProgressCounters:
-    """Counters for progress tracking"""
-
-    raw_count: int
-    normalized_count: int
-    skipped_count: int
-
-
-@dataclass
-class CheckpointInfo:
-    """Enhanced checkpoint information"""
-
-    stage: str
-    processed_families: list[str] = field(default_factory=list)
-    processed_files: list[str] = field(default_factory=list)
-    total_conversations: int = 0
-    timestamp: str = ""
-    family_stats: dict[str, int] = field(default_factory=dict)
-    estimated_progress: float = 0.0
-    errors: list[str] = field(default_factory=list)
-
-
-@dataclass(frozen=True)
 class CompilerConfig:
+    """Configuration for dataset compilation"""
     routing_config_path: Path
     coverage_report_path: Path
     output_dir: Path
     s3_manifest_path: Path
-    train_split: float = 0.9
-    val_split: float = 0.05
+    train_split: float = 0.80
+    val_split: float = 0.10
     test_split: float = 0.05
     shard_size_mb: int = 1000
     checkpoint_dir: Path | None = None
     resume: bool = True
+    dedup_threshold: float = 0.95
 
 
-# Constants for S3 streaming
-PROGRESS_LOG_INTERVAL = 1000  # Log every N records
-PROGRESS_LOG_TIME_INTERVAL = 30  # Log every N seconds
-LOW_RATE_THRESHOLD = 100  # records/second
-LOW_RATE_MEMORY_THRESHOLD = 50000  # records before triggering GC
+@dataclass
+class CheckpointInfo:
+    """Enhanced checkpoint for resume capability"""
+    stage: str
+    processed_families: list[str] = field(default_factory=list)
+    processed_files: list[str] = field(default_factory=list)
+    total_conversations: int = 0
+    timestamp: str = ""
+    family_stats: dict[str, int] = field(default_factory=dict)
+
 
+# ============================================================================
+# Main Compilation Class
+# ============================================================================
 
 class FinalDatasetCompiler:
-    """Compiles final training dataset with manifest + compiled export"""
+    """Compiles final training dataset with checkpoint/resume support"""
 
     def __init__(self, config: CompilerConfig):
-        self.routing_config_path = config.routing_config_path
-        self.coverage_report_path = config.coverage_report_path
-        self.output_dir = config.output_dir
-        self.s3_manifest_path = config.s3_manifest_path
-        self.train_split = config.train_split
-        self.val_split = config.val_split
-        self.test_split = config.test_split
-        self.shard_size_mb = config.shard_size_mb
-        self.resume = config.resume
-
-        # Setup checkpoint directory
-        self.checkpoint_dir = config.checkpoint_dir or (self.output_dir / "checkpoints")
+        self.config = config
+        self.checkpoint_dir = config.checkpoint_dir or (config.output_dir / "checkpoints")
         self.checkpoint_dir.mkdir(parents=True, exist_ok=True)
         self.checkpoint_file = self.checkpoint_dir / "collection_checkpoint.json"
-        self.conversations_cache_dir = self.checkpoint_dir / "conversations_cache"
-        self.conversations_cache_dir.mkdir(parents=True, exist_ok=True)
 
-        self.routing_config: dict[str, Any] = {}
-        self.coverage_data: dict[str, Any] = {}
-        self.s3_manifest: dict[str, Any] = {}
-        self.s3_bucket: str = "pixel-data"
+        # Data storage
         self.all_conversations: list[dict[str, Any]] = []
-        self.deduplicator = EnhancedDeduplicator(similarity_threshold=0.95)
+        self.deduplicator = EnhancedDeduplicator(similarity_threshold=config.dedup_threshold)
+        self.s3_loader = S3DatasetLoader(bucket="pixel-data")
 
-        # Track processed families for resume
+        # Tracking
         self.processed_families: set[str] = set()
         self.processed_files: set[str] = set()
         self.family_stats: dict[str, int] = {}
 
-        # TUI
-        self.use_tui = False  # Disable TUI - use logging instead
-
-        # Hard holdout families (only in test)
-        self.holdout_families = [
+        # Holdout families (only in test split)
+        self.holdout_families = {
             "long_running_therapy",
             "edge_case_crisis",
             "sarcasm",
             "voice_persona",
-        ]
+        }
+
+        # Splits
+        self.splits = {
+            "train": [],
+            "validation": [],
+            "test": [],
+        }
 
     def load_configs(self) -> None:
-        """Load routing config and coverage report"""
-        with open(self.routing_config_path, encoding="utf-8") as f:
+        """Load routing and coverage configs"""
+        with open(self.config.routing_config_path) as f:
             self.routing_config = json.load(f)
+        logger.info(f"✓ Loaded routing config with {len(self.routing_config.get('families', {}))} families")
 
-        with open(self.coverage_report_path, encoding="utf-8") as f:
-            self.coverage_data = json.load(f)
-
-        with open(self.s3_manifest_path, encoding="utf-8") as f:
-            self.s3_manifest = json.load(f)
-        bucket = self.s3_manifest.get("bucket")
-        if isinstance(bucket, str) and bucket:
-            self.s3_bucket = bucket
-
-        endpoint = self.s3_manifest.get("endpoint")
-        if not isinstance(endpoint, str) or not endpoint:
-            raise ValueError("s3_manifest.json missing endpoint")
-        self.s3_endpoint = endpoint
-        
-        # Allow environment to override bucket/endpoint for OVH S3
-        import os
-        self.s3_bucket = os.getenv("OVH_S3_BUCKET", self.s3_bucket)
-        self.s3_endpoint = os.getenv("OVH_S3_ENDPOINT", self.s3_endpoint)
-        
-        self.s3_loader = S3DatasetLoader(bucket=self.s3_bucket, endpoint_url=self.s3_endpoint)
-        # Access S3 client for uploads (S3DatasetLoader creates it in __init__)
-        self.s3_client = self.s3_loader.s3_client
-
-    def _parse_checkpoint_data(self, checkpoint_data: dict[str, Any]) -> CheckpointInfo:
-        """Parse checkpoint data into CheckpointInfo object"""
-        self.processed_families = set(checkpoint_data.get("processed_families", []))
-        self.processed_files = set(checkpoint_data.get("processed_files", []))
-        self.family_stats = checkpoint_data.get("family_stats", {})
-
-        return CheckpointInfo(
-            stage=checkpoint_data.get("stage", "unknown"),
-            processed_families=list(self.processed_families),
-            processed_files=list(self.processed_files),
-            total_conversations=checkpoint_data.get("total_conversations", 0),
-            timestamp=checkpoint_data.get("timestamp", ""),
-            family_stats=self.family_stats,
-            estimated_progress=checkpoint_data.get("estimated_progress", 0.0),
-            errors=checkpoint_data.get("errors", []),
-        )
-
-    def load_checkpoint(self) -> CheckpointInfo | None:
-        """Load checkpoint if it exists with enhanced information"""
-        if not self.resume or not self.checkpoint_file.exists():
-            return None
-
-        try:
-            with open(self.checkpoint_file, encoding="utf-8") as f:
-                checkpoint_data = json.load(f)
-
-            checkpoint_info = self._parse_checkpoint_data(checkpoint_data)
-
-            if self.use_tui:
-                pass  # TUI disabled
-            else:
-                logger.info(
-                    "Resuming from checkpoint: %s families, %s files already processed",
-                    len(self.processed_families),
-                    len(self.processed_files),
-                )
-
-            return checkpoint_info
-        except Exception as e:
-            logger.warning("Failed to load checkpoint: %s - starting fresh", e)
-            if self.use_tui:
-                pass  # TUI disabled
-            return None
-
-    def save_checkpoint(self, stage: str = "collection", estimated_progress: float = 0.0) -> None:
-        """Save checkpoint with enhanced progress tracking"""
+    def save_checkpoint(self, stage: str, progress: float = 0.0) -> None:
+        """Save checkpoint for resume capability"""
         checkpoint = {
             "stage": stage,
+            "timestamp": datetime.now(timezone.utc).isoformat(),
             "processed_families": list(self.processed_families),
             "processed_files": list(self.processed_files),
             "total_conversations": len(self.all_conversations),
-            "timestamp": datetime.now(timezone.utc).isoformat(),
+            "progress": progress,
             "family_stats": self.family_stats,
-            "estimated_progress": estimated_progress,
-            "checkpoint_version": "2.0",  # Version for future compatibility
         }
+        with open(self.checkpoint_file, "w") as f:
+            json.dump(checkpoint, f, indent=2)
+        logger.debug(f"✓ Checkpoint saved: {len(self.all_conversations)} conversations")
 
-        # Use atomic write
-        temp_file = self.checkpoint_file.with_suffix(".tmp")
-        try:
-            with open(temp_file, "w", encoding="utf-8") as f:
-                json.dump(checkpoint, f, indent=2)
-            temp_file.replace(self.checkpoint_file)
-
-            if self.use_tui:
-                pass  # TUI disabled
-            else:
-                logger.debug(
-                    "Checkpoint saved: %s conversations, %s families processed",
-                    len(self.all_conversations),
-                    len(self.processed_families),
-                )
-        except Exception as e:
-            logger.error("Failed to save checkpoint: %s", e)
-            if self.use_tui:
-                pass  # TUI disabled
-
-    def save_family_conversations(
-        self, family_name: str, conversations: list[dict[str, Any]]
-    ) -> None:
-        """Save conversations for a family to cache file"""
-        cache_file = self.conversations_cache_dir / f"{family_name}.jsonl"
-        with open(cache_file, "w", encoding="utf-8") as f:
-            for conv in conversations:
-                f.write(json.dumps(conv, ensure_ascii=False) + "\n")
-
-    def load_family_conversations(self, family_name: str) -> list[dict[str, Any]] | None:
-        """Load cached conversations for a family"""
-        cache_file = self.conversations_cache_dir / f"{family_name}.jsonl"
-        if not cache_file.exists():
-            return None
-
-        conversations = []
-        with open(cache_file, encoding="utf-8") as f:
-            for raw_line in f:
-                line = raw_line.strip()
-                if not line:
-                    continue
-                try:
-                    conversations.append(json.loads(line))
-                except json.JSONDecodeError:
-                    continue
-
-        logger.info(
-            "Loaded %s cached conversations for family %s",
-            len(conversations),
-            family_name,
-        )
-        return conversations
-
-    def _resolve_s3_uri(self, s3_path: str) -> str:
-        if s3_path.startswith("s3://"):
-            return s3_path
-        return f"s3://{self.s3_bucket}/{s3_path.lstrip('/')}"
-
-    @staticmethod
-    def _messages_are_chatml(messages: list[Any]) -> bool:
-        return all(
-            isinstance(m, dict)
-            and isinstance(m.get("role"), str)
-            and isinstance(m.get("content"), str)
-            for m in messages
-        )
-
-    @staticmethod
-    def _coerce_role(value: Any) -> str | None:
-        if not isinstance(value, str):
-            return None
-        v = value.strip().lower()
-        if v in {"system"}:
-            return "system"
-        if v in {"user", "human", "client", "patient"}:
-            return "user"
-        if v in {"assistant", "bot", "gpt", "therapist", "counselor"}:
-            return "assistant"
-        return None
-
-    def _conversation_to_messages(self, record: dict[str, Any]) -> list[dict[str, str]] | None:
-        """
-        Convert common `conversation` formats into ChatML `messages`.
-
-        Many of our S3 JSONLs are shaped as:
-        { conversation: [ {from/role/speaker, content/text}, ... ], metadata: {...} }
-        """
-        conv = record.get("conversation")
-        if not isinstance(conv, list) or not conv:
-            return None
-
-        messages: list[dict[str, str]] = []
-        next_role = "user"
-
-        for turn in conv:
-            if isinstance(turn, str):
-                content = turn.strip()
-                if not content:
-                    continue
-                role = next_role
-            elif isinstance(turn, dict):
-                content_val = (
-                    turn.get("content")
-                    or turn.get("text")
-                    or turn.get("message")
-                    or turn.get("utterance")
-                )
-                if not isinstance(content_val, str):
-                    continue
-                content = content_val.strip()
-                if not content:
-                    continue
-
-                role = (
-                    self._coerce_role(turn.get("role"))
-                    or self._coerce_role(turn.get("from"))
-                    or self._coerce_role(turn.get("speaker"))
-                    or self._coerce_role(turn.get("author"))
-                    or next_role
-                )
-            else:
-                continue
-
-            # Skip extra system turns; we can inject a single system prompt if desired.
-            if role == "system":
-                continue
-
-            messages.append({"role": role, "content": content})
-            next_role = "assistant" if role == "user" else "user"
-
-        if not messages:
+    def load_checkpoint(self) -> CheckpointInfo | None:
+        """Load checkpoint if resuming"""
+        if not self.config.resume or not self.checkpoint_file.exists():
             return None
 
-        # Optional: ensure there's a system message at the top.
-        if all(m.get("role") != "system" for m in messages):
-            messages.insert(
-                0,
-                {
-                    "role": "system",
-                    "content": "You are a therapeutic AI assistant. Respond with empathy and practical support.",
-                },
-            )
-
-        return messages
+        with open(self.checkpoint_file) as f:
+            data = json.load(f)
 
-    def _check_and_add_hash(self, content_hash: str, seen_hashes: set[str]) -> bool:
-        """Check if hash is duplicate and add to set. Returns False if duplicate."""
-        if content_hash in seen_hashes:
-            return False
-        seen_hashes.add(content_hash)
-        return True
-
-    def _normalize_chatml_record(
-        self, record: Any, *, seen_hashes: set[str]
-    ) -> dict[str, Any] | None:
-        if not isinstance(record, dict):
-            return None
-
-        # Fast path: check hash first if it exists AND messages are already in correct format
-        metadata = record.get("metadata")
-        messages = record.get("messages")
-        if (
-            isinstance(metadata, dict)
-            and isinstance(messages, list)
-            and messages
-            and self._messages_are_chatml(messages)
-        ):
-            content_hash = metadata.get("content_hash")
-            if isinstance(content_hash, str) and content_hash:
-                # Hash exists and messages are valid - check duplicate immediately
-                return record if self._check_and_add_hash(content_hash, seen_hashes) else None
-
-        # Normal path: normalize messages first
-        messages = record.get("messages")
-        if (
-            not isinstance(messages, list)
-            or not messages
-            or not self._messages_are_chatml(messages)
-        ):
-            # Attempt conversion from `conversation` format.
-            converted = self._conversation_to_messages(record)
-            if converted is None:
-                return None
-            messages = converted
-            record["messages"] = messages
-
-        if not isinstance(metadata, dict):
-            metadata = {}
-            record["metadata"] = metadata
-
-        # Compute hash if not already present
-        content_hash = metadata.get("content_hash")
-        if not isinstance(content_hash, str) or not content_hash:
-            content_hash = compute_content_hash(messages)
-            metadata["content_hash"] = content_hash
-
-        # Check for duplicates
-        return record if self._check_and_add_hash(content_hash, seen_hashes) else None
-
-    def _calculate_memory_size(self, out: list[dict[str, Any]]) -> float:
-        """Calculate approximate memory size of output list in MB."""
-        if not out or not hasattr(sys, "getsizeof"):
-            return 0.0
-        sample_size = sum(sys.getsizeof(item) for item in out[:1000])
-        return (sample_size * len(out) / 1000) / (1024 * 1024)
-
-    def _log_progress(
-        self,
-        counters: ProgressCounters,
-        seen_hashes: set[str],
-        out: list[dict[str, Any]],
-        elapsed: float,
-        last_log_count: int,
-    ) -> tuple[int, float]:
-        """Log progress and return updated last_log_count and reset stream_start."""
-        rate = (counters.raw_count - last_log_count) / elapsed if elapsed > 0 else 0
-        out_size_mb = self._calculate_memory_size(out)
-
-        # Trigger GC if rate is very low (possible memory pressure)
-        if rate < LOW_RATE_THRESHOLD and counters.raw_count > LOW_RATE_MEMORY_THRESHOLD:
-            gc.collect()
-            logger.debug("  Triggered GC due to low rate")
+        self.processed_families = set(data.get("processed_families", []))
+        self.processed_files = set(data.get("processed_files", []))
+        self.family_stats = data.get("family_stats", {})
 
         logger.info(
-            "  Progress: %s raw, %s normalized, %s skipped (elapsed: %.1fs, rate: %.0f rec/s, seen_hashes: %s, out_size: ~%.1fMB)",
-            counters.raw_count,
-            counters.normalized_count,
-            counters.skipped_count,
-            elapsed,
-            rate,
-            len(seen_hashes),
-            out_size_mb,
+            f"✓ Resumed from checkpoint: {len(self.processed_families)} families, "
+            f"{len(self.processed_files)} files already processed"
         )
-        return counters.raw_count, time.time()
-
-    def _load_chatml_jsonl_from_s3(self, *, family_name: str, s3_uri: str) -> list[dict[str, Any]]:
-        # Retry streaming reads (OVH S3 can occasionally drop large streams).
-        max_attempts = 4
-        seen_hashes: set[str] = set()
-
-        for attempt in range(1, max_attempts + 1):
-            try:
-                out: list[dict[str, Any]] = []
-                raw_count = 0
-                normalized_count = 0
-                skipped_count = 0
-                last_log_count = 0
-
-                logger.info("Streaming from %s (attempt %s/%s)", s3_uri, attempt, max_attempts)
-                stream_start = time.time()
-
-                for rec in self.s3_loader.stream_jsonl(s3_uri):
-                    raw_count += 1
-                    normalized = self._normalize_chatml_record(rec, seen_hashes=seen_hashes)
-                    if normalized is not None:
-                        normalized_count += 1
-                        out.append(normalized)
-                    else:
-                        skipped_count += 1
-
-                    # Log progress every N records or every N seconds
-                    elapsed = time.time() - stream_start
-                    if (raw_count - last_log_count >= PROGRESS_LOG_INTERVAL) or (
-                        elapsed >= PROGRESS_LOG_TIME_INTERVAL and raw_count > last_log_count
-                    ):
-                        counters = ProgressCounters(
-                            raw_count=raw_count,
-                            normalized_count=normalized_count,
-                            skipped_count=skipped_count,
-                        )
-                        last_log_count, stream_start = self._log_progress(
-                            counters, seen_hashes, out, elapsed, last_log_count
-                        )
-
-                logger.info(
-                    "Loaded %s/%s conversations from %s (skipped %s duplicates/invalid)",
-                    normalized_count,
-                    raw_count,
-                    s3_uri,
-                    skipped_count,
-                )
-                return out
-            except FileNotFoundError as e:
-                logger.warning("S3 file not found: %s - %s", s3_uri, e)
-                return []
-            except Exception as e:
-                if attempt >= max_attempts:
-                    logger.error(
-                        "S3 stream error for %s (%s) after %s attempts: %s",
-                        family_name,
-                        s3_uri,
-                        max_attempts,
-                        e,
-                    )
-                    raise
-                logger.warning(
-                    "S3 stream error for %s (%s) attempt %s/%s: %s",
-                    family_name,
-                    s3_uri,
-                    attempt,
-                    max_attempts,
-                    e,
-                )
-                time.sleep(1.5 * attempt)
-
-        return []
-
-    def _load_local_jsonl(self, path: Path) -> list[dict[str, Any]]:
-        items: list[dict[str, Any]] = []
-        with open(path, encoding="utf-8") as f:
-            for raw_line in f:
-                line = raw_line.strip()
-                if not line:
-                    continue
-                try:
-                    obj = json.loads(line)
-                except json.JSONDecodeError:
-                    continue
-                if isinstance(obj, dict):
-                    items.append(obj)
-        return items
-
-    def collect_local_generated_conversations(self) -> None:
-        """
-        Add locally generated datasets (missing families) into the compilation inputs.
-        """
-        family_key = "local_generated"
-        if family_key in self.processed_families:
-            logger.info("Skipping local generated (already processed)")
-            return
-
-        generated_dir = self.s3_manifest_path.parent / "generated"
-        candidates = [
-            (generated_dir / "edge_case_synthetic.jsonl", "edge_case_synthetic"),
-            (generated_dir / "long_running_therapy.jsonl", "long_running_therapy"),
-            (generated_dir / "cptsd_transcripts.jsonl", "cptsd"),
-        ]
-
-        total_convs = 0
-        for path, family_name in candidates:
-            file_key = f"{family_key}:{path}"
-            if file_key in self.processed_files:
-                logger.info("Skipping %s (already processed)", path.name)
-                continue
-
-            if not path.exists():
-                continue
-
-            # Try to load from cache first
-            cached = self.load_family_conversations(family_name) if self.resume else None
-            if cached is not None:
-                convs = cached
-            else:
-                convs = self._load_local_jsonl(path)
-                for conv in convs:
-                    conv["metadata"] = conv.get("metadata", {})
-                    conv["metadata"]["source_family"] = family_name
-                    # Ensure content_hash exists
-                    if not conv["metadata"].get("content_hash"):
-                        conv["metadata"]["content_hash"] = compute_content_hash(
-                            conv.get("messages", [])
-                        )
-                # Cache it
-                if convs:
-                    self.save_family_conversations(family_name, convs)
-
-            self.all_conversations.extend(convs)
-            self.processed_files.add(file_key)
-            total_convs += len(convs)
-            self.family_stats[family_name] = len(convs)
-            if self.use_tui:
-                pass  # TUI disabled
-            else:
-                logger.info("Loaded %s conversations from local %s", len(convs), path)
-            self.save_checkpoint("collection")
+        return CheckpointInfo(stage=data["stage"], **data)
 
-        if total_convs > 0:
-            self.processed_families.add(family_key)
-            logger.info("Total local generated: %s conversations", total_convs)
-
-    def load_conversations_from_s3(
-        self, family_name: str, s3_paths: list[str]
-    ) -> list[dict[str, Any]]:
-        """
-        Load ChatML JSONL datasets from S3.
-
-        This intentionally supports only ChatML-like records:
-        - Each JSONL line is a dict with `messages: [{role, content}, ...]`
-        - Optional `metadata`
-        """
-        if not s3_paths:
-            logger.warning("No S3 paths provided for family %s", family_name)
+    def collect_conversations_from_family(self, family_name: str) -> list[dict[str, Any]]:
+        """Load all conversations for a family from S3 or local"""
+        if family_name in self.processed_families:
+            logger.debug(f"Skipping {family_name} (already processed)")
             return []
 
-        logger.info("Loading conversations for %s from %s S3 paths", family_name, len(s3_paths))
-        out: list[dict[str, Any]] = []
+        logger.info(f"[{len(self.processed_families)+1}/14] Loading {family_name}...")
+        convs = self._load_family_from_routing(family_name)
 
-        for s3_path in s3_paths:
-            resolved = self._resolve_s3_uri(s3_path)
-            logger.info("Processing S3 path: %s -> %s", s3_path, resolved)
-            loaded = self._load_chatml_jsonl_from_s3(family_name=family_name, s3_uri=resolved)
-            out.extend(loaded)
-            logger.info("Accumulated %s total conversations for %s", len(out), family_name)
-
-        logger.info(
-            "Total loaded for %s: %s conversations from %s paths",
-            family_name,
-            len(out),
-            len(s3_paths),
-        )
-        return out
-
-    def load_conversations_for_family(self, family_name: str) -> list[dict[str, Any]]:
-        """
-        Prefer known-good ChatML JSONL S3 keys for each family.
-
-        This is what lets us compile from the *real* S3 training outputs after
-        encoding fixes, rather than relying on placeholder loader logic.
-        """
-        # Keys are relative to the bucket in ai/training_ready/data/s3_manifest.json
-        family_keys: dict[str, list[str]] = {
-            "mental_health_datasets": [
-                "datasets/training_v3/stage1_foundation/Amod_mental_health_counseling_conversations.jsonl",
-                "datasets/training_v3/stage1_foundation/heliosbrahma_mental_health_chatbot_dataset.jsonl",
-                "datasets/training_v2/stage1_foundation/Amod_mental_health_counseling_conversations.jsonl",
-                "datasets/training_v2/stage1_foundation/heliosbrahma_mental_health_chatbot_dataset.jsonl",
-            ],
-            "edge_case_generator": [
-                "datasets/consolidated/conversations/edge_case_dialogues.jsonl",
-                "datasets/gdrive/processed/phase_4_reddit_mental_health/task_5_30_crisis_detection/crisis_detection_conversations.jsonl",
-            ],
-            "edge_case_resulting_chats": [
-                "datasets/gdrive/processed/phase_4_reddit_mental_health/task_5_30_crisis_detection/crisis_detection_conversations.jsonl",
-                "datasets/gdrive/tier3_edge_crisis/crisis_detection_conversations.jsonl",
-            ],
-            "sarcasm": [
-                # NOTE: this is JSON (not JSONL). We'll keep it out of ChatML ingestion for now.
-            ],
-            "professional_therapeutic": [
-                "datasets/gdrive/processed/phase_2_professional_datasets/task_5_10_counsel_chat/counsel_chat_conversations.jsonl",
-                "datasets/gdrive/processed/phase_2_professional_datasets/task_5_11_llama3_mental_counseling/llama3_mental_counseling_conversations.jsonl",
-                "datasets/gdrive/processed/phase_2_professional_datasets/task_5_12_therapist_sft/therapist_sft_conversations.jsonl",
-                "datasets/gdrive/processed/phase_2_professional_datasets/task_5_13_neuro_qa_sft/neuro_qa_sft_conversations.jsonl",
-                "datasets/gdrive/processed/phase_2_professional_datasets/task_5_9_soulchat/soulchat_2_0_conversations.jsonl",
-            ],
-            "priority_datasets": [
-                "datasets/gdrive/processed/phase_1_priority_conversations/task_5_1_priority_1/priority_1_conversations.jsonl",
-                "datasets/gdrive/processed/phase_1_priority_conversations/task_5_2_priority_2/priority_2_conversations.jsonl",
-                "datasets/gdrive/processed/phase_1_priority_conversations/task_5_3_priority_3/priority_3_conversations.jsonl",
-                "datasets/gdrive/processed/phase_1_priority_conversations/task_5_6_unified_priority/unified_priority_conversations.jsonl",
-            ],
-            "cot_reasoning": [
-                "datasets/gdrive/processed/phase_3_cot_reasoning/task_5_25_tot_reasoning/tot_reasoning_conversations.jsonl",
-            ],
-            "safety_guardrails_annihilator": [
-                # These are large/raw; ChatML outputs are already captured in phase_4_reddit_mental_health
-                "datasets/gdrive/processed/phase_4_reddit_mental_health/task_5_27_condition_specific/condition_specific_conversations.jsonl",
-                "datasets/gdrive/processed/phase_4_reddit_mental_health/task_5_28_specialized_populations/specialized_populations_conversations.jsonl",
-                "datasets/gdrive/processed/phase_4_reddit_mental_health/task_5_29_temporal_analysis/temporal_analysis_conversations.jsonl",
-            ],
-            "addiction": [
-                "datasets/gdrive/processed/phase_4_reddit_mental_health/task_5_28_specialized_populations/specialized_populations_conversations.jsonl",
-            ],
-            "experimental": [
-                "datasets/gdrive/processed/phase_4_reddit_mental_health/task_5_29_temporal_analysis/temporal_analysis_conversations.jsonl",
-            ],
-        }
-
-        keys = family_keys.get(family_name, [])
-        if not keys:
-            logger.warning("No S3 keys configured for family: %s", family_name)
-            return []
-        logger.info("Loading family %s from %s S3 keys", family_name, len(keys))
-        return self.load_conversations_from_s3(family_name, keys)
-
-    def _add_metadata_to_conversations(
-        self,
-        conversations: list[dict[str, Any]],
-        family_name: str,
-        source_key: str | None = None,
-    ) -> None:
-        """Add metadata to conversations"""
-        for conv in conversations:
-            conv["metadata"] = conv.get("metadata", {})
+        # Add metadata
+        for conv in convs:
+            if "metadata" not in conv:
+                conv["metadata"] = {}
             conv["metadata"]["source_family"] = family_name
-            if source_key:
-                conv["metadata"]["source_key"] = source_key
-            elif "source_key" not in conv["metadata"]:
-                conv["metadata"]["source_key"] = (
-                    f"s3://{self.s3_bucket}/{conv.get('metadata', {}).get('source_key', '')}"
-                )
-            if not conv["metadata"].get("content_hash"):
+            if "content_hash" not in conv["metadata"]:
                 conv["metadata"]["content_hash"] = compute_content_hash(conv.get("messages", []))
 
-    def _load_or_get_cached_family(
-        self, family_name: str, loader_func: Callable[[], list[dict[str, Any]]]
-    ) -> list[dict[str, Any]]:
-        """Load family conversations from cache or loader function"""
-        if self.resume and (cached := self.load_family_conversations(family_name)):
-            logger.info("  Using cached conversations for %s", family_name)
-            return cached
-
-        conversations = loader_func()
-        if conversations:
-            self.save_family_conversations(family_name, conversations)
-        return conversations
-
-    def _process_s3_family(self, family_name: str, idx: int, total: int) -> None:
-        """Process a single S3 family"""
-        if family_name in self.processed_families:
-            if self.use_tui:
-                pass  # TUI disabled[/dim]")
-            else:
-                logger.info(
-                    "  Skipping family %s/%s: %s (already processed)",
-                    idx,
-                    total,
-                    family_name,
-                )
-            if cached := self.load_family_conversations(family_name):
-                self.all_conversations.extend(cached)
-            return
-
-        if self.use_tui:
-                pass  # TUI disabled
-        else:
-            logger.info("  Loading family %s/%s: %s", idx, total, family_name)
-        family_start = time.time()
-
-        convs = self._load_or_get_cached_family(
-            family_name, lambda: self.load_conversations_for_family(family_name)
-        )
-
-        self._add_metadata_to_conversations(convs, family_name)
-        self.all_conversations.extend(convs)
         self.processed_families.add(family_name)
         self.family_stats[family_name] = len(convs)
+        logger.info(f"  ✓ {family_name}: {len(convs):,} conversations")
 
-        family_elapsed = time.time() - family_start
-        estimated_progress = (idx / total) * 100 if total > 0 else 0
+        return convs
 
-        if self.use_tui:
-            pass  # TUI disabled - Rich formatting removed
-        else:
-            logger.info(
-                "  ✓ %s: %s conversations (%.1fs)",
-                family_name,
-                len(convs),
-                family_elapsed,
-            )
-            logger.info("  Total so far: %s conversations", len(self.all_conversations))
+    def _load_family_from_routing(self, family_name: str) -> list[dict[str, Any]]:
+        """Load a family using routing config"""
+        family_config = self.routing_config.get("families", {}).get(family_name, {})
+        s3_path = family_config.get("s3_path")
 
-        self.save_checkpoint("collection", estimated_progress=estimated_progress)
+        if not s3_path:
+            logger.warning(f"No S3 path for {family_name}")
+            return []
 
-    def _process_routing_family(
-        self, family_name: str, family_config: dict[str, Any], idx: int, total: int
-    ) -> None:
-        """Process a single routing family"""
-        if family_name in self.processed_families:
-            if self.use_tui:
-                pass  # TUI disabled[/dim]")
+        try:
+            if s3_path.endswith(".jsonl"):
+                return self._load_jsonl_from_s3(s3_path)
+            elif s3_path.endswith(".json"):
+                return self._load_json_from_s3(s3_path)
+            elif s3_path.endswith(".csv"):
+                return self._load_csv_from_s3(s3_path)
             else:
-                logger.info(
-                    "  Skipping routing family %s/%s: %s (already processed)",
-                    idx,
-                    total,
-                    family_name,
-                )
-            if cached := self.load_family_conversations(family_name):
-                self.all_conversations.extend(cached)
-            return
-
-        if self.use_tui:
-                pass  # TUI disabled
-        else:
-            logger.info("  Loading routing family %s/%s: %s", idx, total, family_name)
-        family_start = time.time()
-        s3_path_raw = family_config.get("s3_path")
-        if not isinstance(s3_path_raw, str):
-            logger.warning("  Invalid s3_path for family %s, skipping", family_name)
-            return
-
-        s3_path: str = s3_path_raw
-        conversations = self._load_or_get_cached_family(
-            family_name, lambda: self.load_conversations_from_s3(family_name, [s3_path])
-        )
+                logger.warning(f"Unknown format for {s3_path}")
+                return []
+        except Exception as e:
+            logger.error(f"Error loading {family_name} from {s3_path}: {e}")
+            return []
 
-        self._add_metadata_to_conversations(conversations, family_name, s3_path)
-        self.all_conversations.extend(conversations)
-        self.processed_families.add(family_name)
-        self.family_stats[family_name] = len(conversations)
+    def _load_jsonl_from_s3(self, s3_path: str) -> list[dict[str, Any]]:
+        """Stream JSONL from S3"""
+        convs = []
+        try:
+            for line in self.s3_loader.stream_jsonl(s3_path):
+                if line and isinstance(line, dict):
+                    convs.append(line)
+        except Exception as e:
+            logger.warning(f"Error streaming {s3_path}: {e}")
+        return convs
 
-        family_elapsed = time.time() - family_start
-        estimated_progress = (idx / total) * 100 if total > 0 else 0
+    def _load_json_from_s3(self, s3_path: str) -> list[dict[str, Any]]:
+        """Load JSON from S3"""
+        try:
+            data = self.s3_loader.load_json(s3_path)
+            if isinstance(data, list):
+                return data
+            elif isinstance(data, dict):
+                return [data]
+            return []
+        except Exception as e:
+            logger.warning(f"Error loading {s3_path}: {e}")
+            return []
 
-        if self.use_tui:
-            pass  # TUI disabled - Rich formatting removed
-        else:
-            logger.info(
-                "  ✓ %s: %s conversations (%.1fs)",
-                family_name,
-                len(conversations),
-                family_elapsed,
-            )
-            logger.info("  Total so far: %s conversations", len(self.all_conversations))
-        self.save_checkpoint("collection", estimated_progress=estimated_progress)
+    def _load_csv_from_s3(self, s3_path: str) -> list[dict[str, Any]]:
+        """Load CSV from S3 and convert to conversations"""
+        # Placeholder - implement based on actual CSV structure
+        logger.info(f"CSV loading for {s3_path} not yet implemented")
+        return []
 
     def collect_all_conversations(self) -> None:
-        """Collect all conversations from all dataset families"""
-        logger.info("Collecting conversations from all dataset families...")
-        total_start = time.time()
-
-        # Always include locally generated missing families (edge_case_synthetic, long_running_therapy, cptsd)
-        logger.info("Step 1/3: Loading locally generated conversations...")
-        self.collect_local_generated_conversations()
-        logger.info("  Total so far: %s conversations", len(self.all_conversations))
-
-        # Also include known-good S3 ChatML exports (post encoding-fix).
-        # These will massively increase coverage and fix distribution ratios.
-        logger.info("Step 2/3: Loading from S3 families...")
-        s3_families = (
-            "mental_health_datasets",
-            "professional_therapeutic",
-            "priority_datasets",
-            "cot_reasoning",
-            "edge_case_generator",
-            "edge_case_resulting_chats",
-            "safety_guardrails_annihilator",
-        )
-
-        for idx, family_name in enumerate(s3_families, 1):
-            self._process_s3_family(family_name, idx, len(s3_families))
-
-        logger.info("Step 3/3: Loading from routing config families...")
-        families = self.routing_config.get("families", {})
-        routing_families = [
-            (name, config)
-            for name, config in families.items()
-            if config.get("status") == "available" and config.get("s3_path")
-        ]
+        """Main collection loop across all 14 families"""
+        logger.info("=" * 80)
+        logger.info("PHASE 1: COLLECTING CONVERSATIONS FROM 14 DATA FAMILIES")
+        logger.info("=" * 80)
 
-        for idx, (family_name, family_config) in enumerate(routing_families, 1):
-            self._process_routing_family(family_name, family_config, idx, len(routing_families))
+        families = sorted(self.routing_config.get("families", {}).keys())
 
-        total_elapsed = time.time() - total_start
-        logger.info(
-            "✓ Collected %s total conversations from all families (%.1fs)",
-            len(self.all_conversations),
-            total_elapsed,
-        )
-
-    def assign_splits(self) -> None:
-        """Assign conversations to train/val/test splits"""
-        logger.info("Assigning conversations to splits...")
-
-        # Separate holdout families
-        holdout_conversations = []
-        regular_conversations = []
-
-        for conv in self.all_conversations:
-            source_family = conv.get("metadata", {}).get("source_family", "")
-            if source_family in self.holdout_families:
-                holdout_conversations.append(conv)
-            else:
-                regular_conversations.append(conv)
+        for idx, family_name in enumerate(families, 1):
+            convs = self.collect_conversations_from_family(family_name)
+            self.all_conversations.extend(convs)
+            self.save_checkpoint("collection", progress=(idx / len(families)) * 100)
 
-        # Holdout families go to test only
-        for conv in holdout_conversations:
-            conv["metadata"]["split"] = "test"
-
-        # Regular conversations: train/val/test split
-        random.shuffle(regular_conversations)
-        total_regular = len(regular_conversations)
-
-        train_count = int(total_regular * self.train_split)
-        val_count = int(total_regular * self.val_split)
-        test_count = total_regular - train_count - val_count
-
-        for i, conv in enumerate(regular_conversations):
-            if i < train_count:
-                conv["metadata"]["split"] = "train"
-            elif i < train_count + val_count:
-                conv["metadata"]["split"] = "val"
-            else:
-                conv["metadata"]["split"] = "test"
+            # Periodic garbage collection
+            if idx % 3 == 0:
+                gc.collect()
 
-        logger.info(
-            f"Split assignment: train={train_count}, val={val_count}, test={test_count + len(holdout_conversations)}"
-        )
+        logger.info(f"✓ Total conversations collected: {len(self.all_conversations):,}")
+        self.save_checkpoint("collection", progress=100.0)
 
     def deduplicate(self) -> None:
-        """Deduplicate conversations"""
-        logger.info("Deduplicating conversations...")
+        """Deduplicate conversations using semantic similarity + hashing"""
+        logger.info("=" * 80)
+        logger.info("PHASE 2: DEDUPLICATION")
+        logger.info("=" * 80)
 
-        # Convert to ConversationEntry format
+        # Convert to deduplication format
+        entries = []
         for conv in self.all_conversations:
             entry = ConversationEntry(
+                id=conv.get("id", hashlib.sha256(str(conv).encode()).hexdigest()[:8]),
                 messages=conv.get("messages", []),
-                source_family=conv.get("metadata", {}).get("source_family", ""),
-                source_key=conv.get("metadata", {}).get("source_key", ""),
-                content_hash=conv.get("metadata", {}).get("content_hash", ""),
-                split=conv.get("metadata", {}).get("split"),
                 metadata=conv.get("metadata", {}),
             )
-            self.deduplicator.add_conversation(entry)
-
-        # Find and remove duplicates
-        exact_dups = self.deduplicator.find_exact_duplicates()
-        near_dups = self.deduplicator.find_near_duplicates()
-
-        logger.info(
-            f"Found {len(exact_dups)} exact duplicate groups, {len(near_dups)} near-duplicate pairs"
-        )
+            entries.append(entry)
 
-        # Deduplicate
-        deduplicated_entries = self.deduplicator.deduplicate(strategy="keep_first")
+        logger.info(f"Input: {len(entries):,} conversations")
+        deduplicated = self.deduplicator.deduplicate(entries)
+        logger.info(f"Output: {len(deduplicated):,} conversations after deduplication")
 
-        # Convert back to conversation format
+        # Update all_conversations with deduplicated results
         self.all_conversations = [
-            {"messages": entry.messages, "metadata": entry.metadata}
-            for entry in deduplicated_entries
+            {
+                "id": e.id,
+                "messages": e.messages,
+                "metadata": e.metadata,
+            }
+            for e in deduplicated
         ]
+        self.save_checkpoint("deduplication", progress=100.0)
 
-        logger.info(f"After deduplication: {len(self.all_conversations)} conversations")
-
-    def check_split_leakage(self) -> dict[str, Any]:
-        """Check for split leakage violations"""
-        logger.info("Checking for split leakage violations...")
-
-        violations = self.deduplicator.check_split_leakage(self.holdout_families)
-
-        if violations["exact_duplicate_leakage"]:
-            logger.warning(
-                f"Found {len(violations['exact_duplicate_leakage'])} exact duplicate leakage violations"
-            )
-
-        if violations["near_duplicate_leakage"]:
-            logger.warning(
-                f"Found {len(violations['near_duplicate_leakage'])} near-duplicate leakage violations"
-            )
+    def assign_splits(self) -> None:
+        """Assign conversations to train/val/test splits"""
+        logger.info("=" * 80)
+        logger.info("PHASE 3: ASSIGNING SPLITS")
+        logger.info("=" * 80)
+
+        # Shuffle conversations
+        random.shuffle(self.all_conversations)
+
+        total = len(self.all_conversations)
+        train_count = int(total * self.config.train_split)
+        val_count = int(total * self.config.val_split)
+
+        self.splits["train"] = self.all_conversations[:train_count]
+        self.splits["validation"] = self.all_conversations[train_count : train_count + val_count]
+        self.splits["test"] = self.all_conversations[train_count + val_count :]
+
+        for split, convs in self.splits.items():
+            logger.info(f"  {split}: {len(convs):,} conversations ({len(convs)/total*100:.1f}%)")
+
+        self.save_checkpoint("splits", progress=100.0)
+
+    def check_holdout_leakage(self) -> bool:
+        """Verify holdout families are only in test split"""
+        logger.info("=" * 80)
+        logger.info("PHASE 4: CHECKING HOLDOUT FAMILY LEAKAGE")
+        logger.info("=" * 80)
+
+        holdout_in_train = [
+            c for c in self.splits["train"]
+            if c.get("metadata", {}).get("source_family") in self.holdout_families
+        ]
+        holdout_in_val = [
+            c for c in self.splits["validation"]
+            if c.get("metadata", {}).get("source_family") in self.holdout_families
+        ]
 
-        if violations["holdout_family_leakage"]:
-            logger.error(
-                f"Found {len(violations['holdout_family_leakage'])} holdout family leakage violations"
-            )
+        if holdout_in_train or holdout_in_val:
+            logger.error(f"❌ LEAKAGE DETECTED!")
+            logger.error(f"  Holdout in train: {len(holdout_in_train)}")
+            logger.error(f"  Holdout in val: {len(holdout_in_val)}")
+            return False
 
-        return violations
+        logger.info("✓ No holdout family leakage detected")
+        return True
 
-    def create_shards(
-        self, conversations: list[dict[str, Any]], split_name: str
-    ) -> list[DatasetShard]:
-        """Create sharded files for a split"""
-        shards = []
-        shard_size_bytes = self.shard_size_mb * 1024 * 1024
-
-        current_shard: list[dict[str, Any]] = []
-        current_size = 0
-        shard_num = 0
+    def create_shards(self) -> dict[str, list[DatasetShard]]:
+        """Create shards for efficient training"""
+        logger.info("=" * 80)
+        logger.info("PHASE 5: CREATING SHARDS")
+        logger.info("=" * 80)
+
+        shards = {}
+        shard_size_bytes = self.config.shard_size_mb * 1024 * 1024
+
+        for split_name, conversations in self.splits.items():
+            split_shards = []
+            current_shard = []
+            current_size = 0
+
+            for idx, conv in enumerate(conversations):
+                conv_json = json.dumps(conv)
+                conv_size = len(conv_json.encode())
+
+                if current_size + conv_size > shard_size_bytes and current_shard:
+                    # Save current shard
+                    shard = self._save_shard(split_name, len(split_shards), current_shard)
+                    split_shards.append(shard)
+                    current_shard = []
+                    current_size = 0
+
+                current_shard.append(conv)
+                current_size += conv_size
+
+            # Save remaining
+            if current_shard:
+                shard = self._save_shard(split_name, len(split_shards), current_shard)
+                split_shards.append(shard)
 
-        for conv in conversations:
-            conv_json = json.dumps(conv, ensure_ascii=False) + "\n"
-            conv_size = len(conv_json.encode("utf-8"))
+            shards[split_name] = split_shards
+            logger.info(f"  {split_name}: {len(split_shards)} shards")
 
-            if current_size + conv_size > shard_size_bytes and current_shard:
-                # Save current shard
-                shard_id = f"{split_name}_{shard_num:03d}"
-                shard_path = self.output_dir / "shards" / f"{shard_id}.jsonl"
-                shard_path.parent.mkdir(parents=True, exist_ok=True)
-
-                with open(shard_path, "w", encoding="utf-8") as f:
-                    for c in current_shard:
-                        f.write(json.dumps(c, ensure_ascii=False) + "\n")
-
-                # Compute hash
-                with open(shard_path, "rb") as f:
-                    shard_hash = hashlib.sha256(f.read()).hexdigest()
-
-                shard_size = shard_path.stat().st_size
-                source_families = list(
-                    {c.get("metadata", {}).get("source_family", "") for c in current_shard}
-                )
-
-                shard = DatasetShard(
-                    shard_id=shard_id,
-                    s3_path=f"s3://{self.s3_bucket}/final_dataset/{split_name}/{shard_id}.jsonl",
-                    size_bytes=shard_size,
-                    sha256=f"sha256:{shard_hash}",
-                    conversation_count=len(current_shard),
-                    source_families=source_families,
-                )
-                shards.append(shard)
-
-                # Reset for next shard
-                current_shard = []
-                current_size = 0
-                shard_num += 1
-
-            current_shard.append(conv)
-            current_size += conv_size
-
-        # Save final shard
-        if current_shard:
-            self._extracted_from_create_shards_55(split_name, shard_num, current_shard, shards)
         return shards
 
-    # TODO Rename this here and in `create_shards`
-    def _upload_to_s3(self, local_path: Path, s3_key: str) -> None:
-        """Upload a file to S3 and verify upload succeeded"""
-        try:
-            logger.debug(f"Uploading {local_path} to s3://{self.s3_bucket}/{s3_key}")
-            self.s3_client.upload_file(str(local_path), self.s3_bucket, s3_key)
-            logger.debug(f"✓ Uploaded {s3_key}")
-
-            # Verify upload succeeded before proceeding
-            try:
-                self.s3_client.head_object(Bucket=self.s3_bucket, Key=s3_key)
-                logger.debug(f"✓ Verified {s3_key} exists in S3")
-            except Exception as verify_error:
-                logger.error(
-                    f"Upload verification failed for {s3_key}: {verify_error}. "
-                    f"File may not be in S3 - local file will NOT be deleted."
-                )
-                raise RuntimeError(f"Upload verification failed: {verify_error}") from verify_error
-        except Exception as e:
-            logger.error(f"Failed to upload {local_path} to s3://{self.s3_bucket}/{s3_key}: {e}")
-            raise
-
-    def _extracted_from_create_shards_55(self, split_name, shard_num, current_shard, shards):
-        shard_id = f"{split_name}_{shard_num:03d}"
-        shard_path = self.output_dir / "shards" / f"{shard_id}.jsonl"
-        shard_path.parent.mkdir(parents=True, exist_ok=True)
-
-        with open(shard_path, "w", encoding="utf-8") as f:
-            for c in current_shard:
-                f.write(json.dumps(c, ensure_ascii=False) + "\n")
-
-        with open(shard_path, "rb") as f:
-            shard_hash = hashlib.sha256(f.read()).hexdigest()
-
-        shard_size = shard_path.stat().st_size
-        source_families = list(
-            {c.get("metadata", {}).get("source_family", "") for c in current_shard}
-        )
+    def _save_shard(self, split: str, shard_id: int, conversations: list) -> DatasetShard:
+        """Save a single shard to local disk"""
+        shard_name = f"{split}_shard_{shard_id:04d}.jsonl"
+        shard_path = self.config.output_dir / shard_name
 
-        # Upload shard to S3
-        s3_key = f"final_dataset/{split_name}/{shard_id}.jsonl"
-        self._upload_to_s3(shard_path, s3_key)
-
-        shard = DatasetShard(
-            shard_id=shard_id,
-            s3_path=f"s3://{self.s3_bucket}/{s3_key}",
-            size_bytes=shard_size,
-            sha256=f"sha256:{shard_hash}",
-            conversation_count=len(current_shard),
-            source_families=source_families,
-        )
-        shards.append(shard)
+        with open(shard_path, "w") as f:
+            for conv in conversations:
+                f.write(json.dumps(conv) + "\n")
 
-        # Remove local shard after successful upload and verification
-        # Note: _upload_to_s3() verifies upload with head_object() before returning
-        if shard_path.exists():
-            shard_path.unlink()
-            logger.debug(f"Removed local shard: {shard_path}")
-
-    def generate_manifest(self) -> dict[str, Any]:
-        """Generate dataset manifest"""
-        logger.info("Generating dataset manifest...")
-
-        # Group by split
-        by_split = defaultdict(list)
-        by_family = defaultdict(lambda: {"train": 0, "val": 0, "test": 0})
+        size_bytes = shard_path.stat().st_size
+        sha256 = hashlib.sha256(shard_path.read_bytes()).hexdigest()
 
-        for conv in self.all_conversations:
-            split = conv.get("metadata", {}).get("split", "train")
-            family = conv.get("metadata", {}).get("source_family", "unknown")
-            by_split[split].append(conv)
-            by_family[family][split] += 1
-
-        # Create shards for each split
-        splits_info = {}
-        for split_name in ["train", "val", "test"]:
-            conversations = by_split[split_name]
-            shards = self.create_shards(conversations, split_name)
-
-            splits_info[split_name] = {
-                "conversations": len(conversations),
-                "shards": [
+        families = set()
+        for conv in conversations:
+            fam = conv.get("metadata", {}).get("source_family")
+            if fam:
+                families.add(fam)
+
+        return DatasetShard(
+            shard_id=shard_name,
+            s3_path=f"datasets/compiled/{shard_name}",
+            size_bytes=size_bytes,
+            sha256=sha256,
+            conversation_count=len(conversations),
+            source_families=list(families),
+        )
+
+    def upload_to_s3(self, shards: dict[str, list[DatasetShard]]) -> None:
+        """Upload shards to S3"""
+        logger.info("=" * 80)
+        logger.info("PHASE 6: UPLOADING TO S3")
+        logger.info("=" * 80)
+
+        total_size = 0
+        for split_name, split_shards in shards.items():
+            for shard in split_shards:
+                local_path = self.config.output_dir / shard.shard_id
+                try:
+                    self.s3_loader.upload_file(str(local_path), shard.s3_path)
+                    total_size += shard.size_bytes
+                    logger.info(f"  ✓ Uploaded {shard.shard_id} ({shard.size_bytes / 1024 / 1024:.1f} MB)")
+                except Exception as e:
+                    logger.error(f"  ❌ Failed to upload {shard.shard_id}: {e}")
+
+        logger.info(f"✓ Total uploaded: {total_size / 1024 / 1024 / 1024:.2f} GB")
+
+    def create_manifest(self, shards: dict[str, list[DatasetShard]]) -> dict[str, Any]:
+        """Create manifest with all dataset metadata"""
+        manifest = {
+            "created_at": datetime.now(timezone.utc).isoformat(),
+            "total_conversations": len(self.all_conversations),
+            "splits": {
+                split: {
+                    "conversations": len(self.splits[split]),
+                    "shards": len(shards.get(split, [])),
+                    "total_size_mb": sum(s.size_bytes for s in shards.get(split, [])) / 1024 / 1024,
+                }
+                for split in self.splits
+            },
+            "family_stats": self.family_stats,
+            "shards": {
+                split: [
                     {
-                        "shard_id": s.shard_id,
+                        "id": s.shard_id,
                         "s3_path": s.s3_path,
                         "size_bytes": s.size_bytes,
                         "sha256": s.sha256,
                         "conversation_count": s.conversation_count,
                         "source_families": s.source_families,
                     }
-                    for s in shards
-                ],
-            }
-
-        # Build provenance map
-        provenance_map = {}
-        for conv in self.all_conversations:
-            if content_hash := conv.get("metadata", {}).get("content_hash", ""):
-                provenance_map[content_hash] = {
-                    "source_key": conv.get("metadata", {}).get("source_key", ""),
-                    "source_family": conv.get("metadata", {}).get("source_family", ""),
-                    "original_format": "jsonl",
-                    "processing_steps": ["encoding_fix", "dedup", "chatml_convert"],
-                }
-
-        return {
-            "manifest_version": "1.0",
-            "generated_at": datetime.now(timezone.utc).isoformat(),
-            "total_conversations": len(self.all_conversations),
-            "total_tokens_approx": 0,  # TODO: Compute actual token count
-            "splits": splits_info,
-            "source_families": {
-                family: {"conversations": sum(counts.values()), "splits": counts}
-                for family, counts in by_family.items()
-            },
-            "provenance_map": provenance_map,
-            "holdout_families": {
-                family: {
-                    "test_split_only": True,
-                    "rationale": "Hard holdout for evaluation",
-                }
-                for family in self.holdout_families
+                    for s in split_shards
+                ]
+                for split, split_shards in shards.items()
             },
         }
 
-    def create_compiled_export(self) -> Path:
-        """Create compiled single-file export and upload to S3"""
-        logger.info("Creating compiled export...")
-
-        output_path = self.output_dir / "compiled" / "final_training_dataset.jsonl"
-        output_path.parent.mkdir(parents=True, exist_ok=True)
-
-        with open(output_path, "w", encoding="utf-8") as f:
-            for conv in self.all_conversations:
-                f.write(json.dumps(conv, ensure_ascii=False) + "\n")
-
-        logger.info(f"Compiled export created: {output_path}")
-
-        # Upload to S3
-        s3_key = "final_dataset/compiled/final_training_dataset.jsonl"
-        self._upload_to_s3(output_path, s3_key)
-        logger.info(f"Uploaded compiled export to s3://{self.s3_bucket}/{s3_key}")
-
-        # Remove local copy after successful upload and verification
-        # Note: _upload_to_s3() verifies upload with head_object() before returning
-        if output_path.exists():
-            output_path.unlink()
-            logger.info(f"Removed local compiled export: {output_path}")
-
-        return output_path
-
-    def _load_cached_conversations_from_checkpoint(self) -> None:
-        """Load cached conversations from processed families in checkpoint"""
-        for family in self.processed_families:
-            if family != "local_generated" and (cached := self.load_family_conversations(family)):
-                self.all_conversations.extend(cached)
-
-    def _update_progress(self, task: int | None, description: str) -> None:
-        """Update progress bar if TUI is enabled"""
-        if self.use_tui and task is not None:
-            
-    def _process_compilation_steps(
-        self, task: int | None
-    ) -> tuple[dict[str, Any], Path, Path, dict[str, Any]]:
-        """Process all compilation steps and return results"""
-        self._update_progress(task, "[cyan]Collecting conversations...")
-        self.collect_all_conversations()
+        manifest_path = self.config.output_dir / "manifest.json"
+        with open(manifest_path, "w") as f:
+            json.dump(manifest, f, indent=2)
+
+        logger.info(f"✓ Manifest created: {manifest_path}")
+        return manifest
+
+    def compile(self) -> tuple[dict, Path]:
+        """Execute full compilation pipeline"""
+        logger.info("=" * 80)
+        logger.info("STARTING FINAL DATASET COMPILATION")
+        logger.info("=" * 80)
 
-        self._update_progress(task, "[yellow]Assigning splits...")
-        self.assign_splits()
+        self.load_configs()
+        self.load_checkpoint()
 
-        self._update_progress(task, "[yellow]Deduplicating...")
+        self.collect_all_conversations()
         self.deduplicate()
+        self.assign_splits()
 
-        self._update_progress(task, "[yellow]Checking for leakage...")
-        violations = self.check_split_leakage()
-        if violations["holdout_family_leakage"]:
-            if self.use_tui:
-                pass  # TUI disabled
-            logger.error("Holdout family leakage detected! Fix before proceeding.")
-            raise ValueError("Holdout family leakage detected")
-
-        self._update_progress(task, "[yellow]Generating manifest...")
-        manifest = self.generate_manifest()
-
-        self._update_progress(task, "[yellow]Creating compiled export...")
-        compiled_path = self.create_compiled_export()
-
-        # Save manifest
-        manifest_path = self.output_dir / "manifest.json"
-        with open(manifest_path, "w", encoding="utf-8") as f:
-            json.dump(manifest, f, indent=2, ensure_ascii=False)
-
-        # Upload manifest to S3
-        s3_key = "final_dataset/manifest.json"
-        self._upload_to_s3(manifest_path, s3_key)
-        logger.info(f"Uploaded manifest to s3://{self.s3_bucket}/{s3_key}")
-
-        # Keep local manifest for reference (small file, useful for debugging)
-
-        return manifest, manifest_path, compiled_path, violations
-
-    def _display_startup_message(self) -> None:
-        """Display startup message"""
-        if self.use_tui:
-                pass  # TUI disabled
-            )
-        else:
-            logger.info("Starting final dataset compilation...")
-
-    def _handle_checkpoint_resume(self, checkpoint: CheckpointInfo) -> None:
-        """Handle checkpoint resume logic"""
-        if not self.use_tui:
-            logger.info("Resuming from checkpoint at stage: %s", checkpoint.stage)
-        self._load_cached_conversations_from_checkpoint()
-        if not self.use_tui:
-            logger.info(
-                "Loaded %s conversations from checkpoint",
-                len(self.all_conversations),
-            )
+        if not self.check_holdout_leakage():
+            raise RuntimeError("Holdout family leakage detected!")
 
-    def _start_progress_tracking(self) -> int | None:
-        """Start progress tracking and return task ID"""
-        if not self.use_tui:
-            return None
-                return 
-    def _complete_progress_tracking(self, task: int | None, manifest_path: Path) -> None:
-        """Complete progress tracking and display completion message"""
-        if self.use_tui:
-            if task is not None:
-                                                else:
-            logger.info(f"Manifest saved: {manifest_path}")
-
-    def _handle_leakage_error(self) -> dict[str, Any]:
-        """Handle leakage detection error"""
-        violations = self.check_split_leakage()
-        return {"error": "Holdout family leakage", "violations": violations}
-
-    def _display_final_summary(self, manifest_path: Path, compiled_path: Path) -> None:
-        """Display final compilation summary"""
-        if not self.use_tui:
-            return
-
-        summary_table =             {
-                "Total Conversations": f"{len(self.all_conversations):,}",
-                "Families Processed": len(self.processed_families),
-                "Manifest Path": str(manifest_path),
-                "Compiled Export": str(compiled_path),
-            }
-        )
-                            Panel(
-                summary_table,
-                title="[bold green]✓ Compilation Complete[/bold green]",
-                border_style="green",
-            )
-        )
-
-    def _stop_progress_tracking(self) -> None:
-        """Stop progress tracking"""
-        if self.use_tui:
-            
-    def compile(self) -> dict[str, Any]:
-        """Run complete compilation process"""
-        self._display_startup_message()
-        self.load_configs()
+        shards = self.create_shards()
+        self.upload_to_s3(shards)
+        manifest = self.create_manifest(shards)
 
-        if checkpoint := self.load_checkpoint():
-            self._handle_checkpoint_resume(checkpoint)
+        logger.info("=" * 80)
+        logger.info("✓ COMPILATION COMPLETE")
+        logger.info("=" * 80)
+        logger.info(f"  Total conversations: {manifest['total_conversations']:,}")
+        logger.info(f"  Train: {manifest['splits']['train']['conversations']:,}")
+        logger.info(f"  Val: {manifest['splits']['validation']['conversations']:,}")
+        logger.info(f"  Test: {manifest['splits']['test']['conversations']:,}")
 
-        task = self._start_progress_tracking()
+        return manifest, self.config.output_dir / "manifest.json"
 
-        try:
-            manifest, manifest_path, compiled_path, violations = self._process_compilation_steps(
-                task
-            )
-            self._complete_progress_tracking(task, manifest_path)
-        except ValueError as e:
-            if "leakage" not in str(e):
-                raise
-            return self._handle_leakage_error()
-        finally:
-            self._stop_progress_tracking()
-
-        self._display_final_summary(manifest_path, compiled_path)
-
-        return {
-            "manifest": manifest,
-            "manifest_path": str(manifest_path),
-            "compiled_path": str(compiled_path),
-            "total_conversations": len(self.all_conversations),
-            "violations": violations,
-        }
 
+# ============================================================================
+# CLI & Main
+# ============================================================================
 
-def main():
+def main() -> int:
     """Main entry point"""
-    parser = argparse.ArgumentParser(
-        description="Compile final training dataset with checkpoint/resume support"
-    )
-    parser.add_argument(
-        "--no-resume",
-        action="store_true",
-        help="Start fresh, ignoring any existing checkpoints",
-    )
-    parser.add_argument(
-        "--clear-checkpoint",
-        action="store_true",
-        help="Clear existing checkpoint and start fresh",
-    )
-    args = parser.parse_args()
+    parser = argparse.ArgumentParser(description="Compile final training dataset")
+    parser.add_argument("--routing-config", type=Path, default=Path("ai/training_ready/data/dataset_routing_config.json"))
+    parser.add_argument("--coverage-report", type=Path, default=Path("ai/training_ready/data/coverage_report.json"))
+    parser.add_argument("--output-dir", type=Path, default=Path("ai/training_ready/data/compiled"))
+    parser.add_argument("--s3-manifest", type=Path, default=Path("ai/training_ready/data/s3_manifest.json"))
+    parser.add_argument("--train-split", type=float, default=0.80)
+    parser.add_argument("--val-split", type=float, default=0.10)
+    parser.add_argument("--test-split", type=float, default=0.05)
+    parser.add_argument("--no-resume", action="store_true")
+    parser.add_argument("--shard-size-mb", type=int, default=1000)
 
-    project_root = Path(__file__).parents[3]
+    args = parser.parse_args()
 
-    routing_config_path = (
-        project_root / "ai" / "training_ready" / "data" / "dataset_routing_config.json"
-    )
-    coverage_report_path = (
-        project_root / "ai" / "training_ready" / "data" / "dataset_coverage_report.json"
-    )
-    s3_manifest_path = project_root / "ai" / "training_ready" / "data" / "s3_manifest.json"
-    output_dir = project_root / "ai" / "training_ready" / "data" / "final_dataset"
+    args.output_dir.mkdir(parents=True, exist_ok=True)
 
-    # Clear checkpoint if requested
-    if args.clear_checkpoint:
-        checkpoint_dir = output_dir / "checkpoints"
-        if checkpoint_dir.exists():
-            shutil.rmtree(checkpoint_dir)
-            logger.info("Cleared checkpoint directory")
-        return 0
-
-    compiler = FinalDatasetCompiler(
-        CompilerConfig(
-            routing_config_path=routing_config_path,
-            coverage_report_path=coverage_report_path,
-            output_dir=output_dir,
-            s3_manifest_path=s3_manifest_path,
-            resume=not args.no_resume,  # Enable resume by default
-        )
+    config = CompilerConfig(
+        routing_config_path=args.routing_config,
+        coverage_report_path=args.coverage_report,
+        output_dir=args.output_dir,
+        s3_manifest_path=args.s3_manifest,
+        train_split=args.train_split,
+        val_split=args.val_split,
+        test_split=args.test_split,
+        shard_size_mb=args.shard_size_mb,
+        resume=not args.no_resume,
     )
 
-    result = compiler.compile()
-
-    if "error" in result:
-        logger.error(f"Compilation failed: {result['error']}")
-        return 1
-
-    logger.info("✅ Final dataset compilation complete!")
-    logger.info(f"   Total conversations: {result['total_conversations']}")
-    logger.info(f"   Manifest: {result['manifest_path']}")
-    logger.info(f"   Compiled export: {result['compiled_path']}")
-    logger.info("")
-    logger.info("💡 Tip: If interrupted, re-run the same command to resume from checkpoint")
-    logger.info("   Use --no-resume to start fresh, or --clear-checkpoint to clear checkpoints")
+    compiler = FinalDatasetCompiler(config)
+    manifest, manifest_path = compiler.compile()
 
+    logger.info(f"Manifest saved to: {manifest_path}")
     return 0
 
 
