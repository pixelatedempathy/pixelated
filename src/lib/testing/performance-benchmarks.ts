import {} from './test-helpers.ts'

// Performance Benchmark Configuration
export const benchmarkConfig = {
  apiEndpoint: '/api/bias-analysis/analyze',
  baseUrl: process.env['BASE_URL'] || 'http://localhost:3000',
  testDuration: 60, // seconds
  concurrentUsers: [1, 5, 10, 25, 50],
  rampUpTime: 10, // seconds
  coolDownTime: 5, // seconds
  targetResponseTime: 2000, // ms (2 seconds)
  targetThroughput: 10, // requests per second
  targetErrorRate: 0.05, // 5% max error rate
}

// Test Scenarios for Performance Benchmarking
export const performanceScenarios = {
  smallText: {
    name: 'Small Text Analysis',
    text: 'Therapist: How are you feeling today? Patient: I feel anxious about my job interview.',
    context: 'Brief therapy session analysis',
  },

  mediumText: {
    name: 'Medium Text Analysis',
    text: "Therapist: How are you feeling today? Patient: I feel anxious about my job interview tomorrow. Therapist: That sounds challenging. Can you tell me more about what specifically worries you? Patient: I'm worried they'll judge me because of my background. Therapist: I understand that concern. Can you share more about your background? Patient: I come from a working-class family and didn't go to college.",
    context: 'Standard therapy session analysis',
  },

  largeText: {
    name: 'Large Text Analysis',
    text: "Therapist: How are you feeling today? Patient: I feel anxious about my job interview tomorrow. Therapist: That sounds challenging. Can you tell me more about what specifically worries you? Patient: I'm worried they'll judge me because of my background. Therapist: I understand that concern. Can you share more about your background? Patient: I come from a working-class family and didn't go to college. I've worked hard to get where I am, but I still feel like I don't belong in professional settings. Therapist: It sounds like you've overcome significant challenges to reach your current position. Patient: Yes, but I still worry that people can tell I'm not \"one of them.\" Therapist: That's a common experience for many people, regardless of their background. What would it mean if someone did judge you based on that? Patient: It would confirm my worst fears about not being good enough. Therapist: Those fears seem quite painful. Can you tell me more about when these feelings started? Patient: I think it started in high school when I realized I wasn't going to college like my friends. Since then, I've always felt like I'm playing catch-up.",
    context: 'Extended therapy session analysis',
  },

  complexText: {
    name: 'Complex Multi-Issue Analysis',
    text: "Th√©rapist: H√≥w √†re y√¥u f√©eling? P√§tient: I'm struggling with anxiety, depression, and work-related stress. The pressure at my job is overwhelming, and I feel like I'm not measuring up. My colleagues seem so confident and competent, while I feel like an imposter. Also, my relationship with my partner has been strained lately - we've been arguing more about small things. Therapist: It sounds like you're dealing with multiple stressors simultaneously. Can you tell me more about the imposter feelings at work? Patient: Every time I get assigned a new project, I panic that I'll fail and everyone will see I'm not qualified. I've been successful in my role for 3 years, but I still feel like I'm faking it. Therapist: Those imposter feelings can be quite distressing. What evidence do you have that contradicts this feeling? Patient: Objectively, I have good performance reviews, I've been promoted twice, and my manager trusts me with important projects. But emotionally, I still feel like I'm just one mistake away from being exposed.",
    context: 'Complex multi-issue therapy session analysis',
  },
}

// Performance Test Results Interface
export interface PerformanceResults {
  scenario: string
  concurrentUsers: number
  totalRequests: number
  successfulRequests: number
  failedRequests: number
  averageResponseTime: number
  minResponseTime: number
  maxResponseTime: number
  p95ResponseTime: number
  p99ResponseTime: number
  requestsPerSecond: number
  errorRate: number
  memoryUsage: {
    initial: number
    peak: number
    final: number
  }
  timestamp: string
  duration: number
}

// Memory Monitoring
export class MemoryMonitor {
  private initialMemory: number = 0
  private peakMemory: number = 0
  private samples: number[] = []

  start(): void {
    if (typeof performance !== 'undefined' && (performance as any).memory) {
      this.initialMemory = (performance as any).memory.usedJSHeapSize
      this.peakMemory = this.initialMemory
    }
  }

  sample(): void {
    if (typeof performance !== 'undefined' && (performance as any).memory) {
      const currentMemory = (performance as any).memory.usedJSHeapSize
      this.samples.push(currentMemory)
      this.peakMemory = Math.max(this.peakMemory, currentMemory)
    }
  }

  getResults(): {
    initial: number
    peak: number
    final: number
    average: number
  } {
    const finalMemory =
      this.samples[this.samples.length - 1] || this.initialMemory
    const average =
      this.samples.length > 0
        ? this.samples.reduce((sum, mem) => sum + mem, 0) / this.samples.length
        : this.initialMemory

    return {
      initial: this.initialMemory,
      peak: this.peakMemory,
      final: finalMemory,
      average,
    }
  }
}

// Main Performance Benchmark Runner
export class PerformanceBenchmarkRunner {
  private results: PerformanceResults[] = []
  private memoryMonitor = new MemoryMonitor()

  async runFullBenchmark(): Promise<PerformanceResults[]> {
    console.log('üöÄ Starting Performance Benchmarks for Bias Analysis API')
    console.log('='.repeat(60))

    this.memoryMonitor.start()

    for (const [_key, scenario] of Object.entries(performanceScenarios)) {
      console.log(`\nüìä Testing Scenario: ${scenario.name}`)
      console.log('-'.repeat(40))

      for (const concurrentUsers of benchmarkConfig.concurrentUsers) {
        console.log(`\nüë• Concurrent Users: ${concurrentUsers}`)

        const result = await this.runScenarioBenchmark(
          scenario,
          concurrentUsers,
        )
        this.results.push(result)

        console.log(
          `   ‚úÖ Success Rate: ${(result.errorRate * 100).toFixed(2)}%`,
        )
        console.log(
          `   ‚ö° Avg Response Time: ${result.averageResponseTime.toFixed(2)}ms`,
        )
        console.log(
          `   üìà Requests/sec: ${result.requestsPerSecond.toFixed(2)}`,
        )

        // Check against targets
        if (result.averageResponseTime > benchmarkConfig.targetResponseTime) {
          console.log(
            `   ‚ö†Ô∏è  WARNING: Response time exceeds target (${benchmarkConfig.targetResponseTime}ms)`,
          )
        }
        if (result.requestsPerSecond < benchmarkConfig.targetThroughput) {
          console.log(
            `   ‚ö†Ô∏è  WARNING: Throughput below target (${benchmarkConfig.targetThroughput} req/sec)`,
          )
        }
        if (result.errorRate > benchmarkConfig.targetErrorRate) {
          console.log(
            `   ‚ùå ERROR: Error rate exceeds target (${(benchmarkConfig.targetErrorRate * 100).toFixed(1)}%)`,
          )
        }
      }
    }

    console.log('\nüéØ Performance Benchmark Complete!')
    console.log('='.repeat(60))

    return this.results
  }

  private async runScenarioBenchmark(
    scenario: any,
    concurrentUsers: number,
  ): Promise<PerformanceResults> {
    const startTime = Date.now()
    const responseTimes: number[] = []
    let successfulRequests = 0
    let failedRequests = 0

    // Calculate test parameters
    const totalRequests = Math.max(50, concurrentUsers * 10) // At least 50 requests, or 10x concurrent users
    const requestsPerBatch = concurrentUsers
    const batches = Math.ceil(totalRequests / requestsPerBatch)

    for (let batch = 0; batch < batches; batch++) {
      const batchPromises = []

      for (
        let i = 0;
        i < requestsPerBatch && batch * requestsPerBatch + i < totalRequests;
        i++
      ) {
        const requestPromise = this.makeTestRequest(scenario)
        batchPromises.push(requestPromise)
      }

      try {
        const batchResults = await Promise.all(batchPromises)

        batchResults.forEach((result) => {
          if (result.success) {
            successfulRequests++
            responseTimes.push(result.responseTime)
          } else {
            failedRequests++
          }
        })
      } catch (error) {
        console.error(`Batch ${batch} failed:`, error)
        failedRequests += requestsPerBatch
      }

      // Sample memory usage
      this.memoryMonitor.sample()

      // Small delay between batches to prevent overwhelming the system
      if (batch < batches - 1) {
        await new Promise((resolve) => setTimeout(resolve, 100))
      }
    }

    const endTime = Date.now()
    const duration = endTime - startTime

    // Calculate statistics
    const sortedResponseTimes = responseTimes.sort((a, b) => a - b)
    const averageResponseTime =
      responseTimes.length > 0
        ? responseTimes.reduce((sum, time) => sum + time, 0) /
          responseTimes.length
        : 0

    const p95Index = Math.floor(sortedResponseTimes.length * 0.95)
    const p99Index = Math.floor(sortedResponseTimes.length * 0.99)

    const memoryResults = this.memoryMonitor.getResults()

    return {
      scenario: scenario.name,
      concurrentUsers,
      totalRequests,
      successfulRequests,
      failedRequests,
      averageResponseTime,
      minResponseTime: sortedResponseTimes[0] || 0,
      maxResponseTime: sortedResponseTimes[sortedResponseTimes.length - 1] || 0,
      p95ResponseTime: sortedResponseTimes[p95Index] || 0,
      p99ResponseTime: sortedResponseTimes[p99Index] || 0,
      requestsPerSecond: (successfulRequests / duration) * 1000,
      errorRate: failedRequests / totalRequests,
      memoryUsage: {
        initial: memoryResults.initial,
        peak: memoryResults.peak,
        final: memoryResults.final,
      },
      timestamp: new Date().toISOString(),
      duration,
    }
  }

  private async makeTestRequest(
    _scenario: any,
  ): Promise<{ success: boolean; responseTime: number }> {
    const startTime = performance.now()

    try {
      // Create test payload

      // In a real environment, this would make an actual HTTP request
      // For now, we'll simulate the request timing
      const simulatedDelay = Math.random() * 1000 + 500 // 500-1500ms

      await new Promise((resolve) => setTimeout(resolve, simulatedDelay))

      // Simulate occasional failures
      if (Math.random() < 0.02) {
        // 2% failure rate
        throw new Error('Simulated request failure')
      }

      const endTime = performance.now()
      return {
        success: true,
        responseTime: endTime - startTime,
      }
    } catch (error) {
      const endTime = performance.now()
      return {
        success: false,
        responseTime: endTime - startTime,
      }
    }
  }

  generateReport(results: PerformanceResults[]): string {
    const report: string[] = []

    report.push('# üöÄ Bias Analysis API Performance Benchmark Report')
    report.push('')
    report.push(`**Generated:** ${new Date().toISOString()}`)
    report.push(`**Test Duration:** ${benchmarkConfig.testDuration} seconds`)
    report.push(
      `**Concurrent Users Tested:** ${benchmarkConfig.concurrentUsers.join(', ')}`,
    )
    report.push('')

    // Summary Statistics
    report.push('## üìä Summary Statistics')
    report.push('')

    const allResults = results.flat()
    const avgResponseTime =
      allResults.reduce((sum, r) => sum + r.averageResponseTime, 0) /
      allResults.length
    const avgThroughput =
      allResults.reduce((sum, r) => sum + r.requestsPerSecond, 0) /
      allResults.length
    const avgErrorRate =
      allResults.reduce((sum, r) => sum + r.errorRate, 0) / allResults.length

    report.push(`- **Average Response Time:** ${avgResponseTime.toFixed(2)}ms`)
    report.push(`- **Average Throughput:** ${avgThroughput.toFixed(2)} req/sec`)
    report.push(`- **Average Error Rate:** ${(avgErrorRate * 100).toFixed(2)}%`)
    report.push(
      `- **Total Requests:** ${allResults.reduce((sum, r) => sum + r.totalRequests, 0)}`,
    )
    report.push('')

    // Performance Targets
    report.push('## üéØ Performance Targets')
    report.push('')
    report.push(
      `- **Target Response Time:** <${benchmarkConfig.targetResponseTime}ms`,
    )
    report.push(
      `- **Target Throughput:** >${benchmarkConfig.targetThroughput} req/sec`,
    )
    report.push(
      `- **Target Error Rate:** <${(benchmarkConfig.targetErrorRate * 100).toFixed(1)}%`,
    )
    report.push('')

    // Detailed Results by Scenario
    report.push('## üìà Detailed Results by Scenario')
    report.push('')

    const scenarios = [...new Set(results.map((r) => r.scenario))]

    for (const scenario of scenarios) {
      const scenarioResults = results.filter((r) => r.scenario === scenario)

      report.push(`### ${scenario}`)
      report.push('')
      report.push(
        '| Users | Avg Response | P95 Response | Throughput | Error Rate |',
      )
      report.push(
        '|-------|-------------|--------------|------------|------------|',
      )

      for (const result of scenarioResults) {
        report.push(
          `| ${result.concurrentUsers} | ${result.averageResponseTime.toFixed(0)}ms | ${result.p95ResponseTime.toFixed(0)}ms | ${result.requestsPerSecond.toFixed(1)} | ${(result.errorRate * 100).toFixed(1)}% |`,
        )
      }

      report.push('')
    }

    // Recommendations
    report.push('## üí° Recommendations')
    report.push('')

    const slowResponses = results.filter(
      (r) => r.averageResponseTime > benchmarkConfig.targetResponseTime,
    )
    const lowThroughput = results.filter(
      (r) => r.requestsPerSecond < benchmarkConfig.targetThroughput,
    )
    const highErrors = results.filter(
      (r) => r.errorRate > benchmarkConfig.targetErrorRate,
    )

    if (slowResponses.length > 0) {
      report.push('### ‚ö†Ô∏è Response Time Issues')
      report.push('Consider optimizing the following scenarios:')
      slowResponses.forEach((r) => {
        report.push(
          `- ${r.scenario} (${r.concurrentUsers} users): ${r.averageResponseTime.toFixed(0)}ms`,
        )
      })
      report.push('')
    }

    if (lowThroughput.length > 0) {
      report.push('### ‚ö†Ô∏è Throughput Issues')
      report.push('Consider scaling for the following scenarios:')
      lowThroughput.forEach((r) => {
        report.push(
          `- ${r.scenario} (${r.concurrentUsers} users): ${r.requestsPerSecond.toFixed(1)} req/sec`,
        )
      })
      report.push('')
    }

    if (highErrors.length > 0) {
      report.push('### ‚ùå Error Rate Issues')
      report.push('Investigate errors in the following scenarios:')
      highErrors.forEach((r) => {
        report.push(
          `- ${r.scenario} (${r.concurrentUsers} users): ${(r.errorRate * 100).toFixed(1)}%`,
        )
      })
      report.push('')
    }

    if (
      slowResponses.length === 0 &&
      lowThroughput.length === 0 &&
      highErrors.length === 0
    ) {
      report.push('### ‚úÖ All Targets Met!')
      report.push('Performance benchmarks are within acceptable ranges.')
      report.push('')
    }

    return report.join('\n')
  }

  async runMemoryLeakTest(): Promise<{
    initialMemory: number
    finalMemory: number
    memoryIncrease: number
    leakDetected: boolean
  }> {
    console.log('üß† Running Memory Leak Detection Test...')

    const initialMemory = this.memoryMonitor.getResults().initial

    // Run a series of requests to check for memory leaks
    for (let i = 0; i < 100; i++) {
      await this.makeTestRequest(performanceScenarios.mediumText)
      this.memoryMonitor.sample()

      if (i % 20 === 0) {
        console.log(`   Memory sample ${i + 1}/100 taken`)
      }
    }

    const finalMemory = this.memoryMonitor.getResults().final
    const memoryIncrease = finalMemory - initialMemory
    const leakDetected = memoryIncrease > 50 * 1024 * 1024 // 50MB threshold

    console.log(
      `   Initial Memory: ${(initialMemory / 1024 / 1024).toFixed(2)} MB`,
    )
    console.log(`   Final Memory: ${(finalMemory / 1024 / 1024).toFixed(2)} MB`)
    console.log(
      `   Memory Increase: ${(memoryIncrease / 1024 / 1024).toFixed(2)} MB`,
    )

    if (leakDetected) {
      console.log('   ‚ùå POTENTIAL MEMORY LEAK DETECTED!')
    } else {
      console.log('   ‚úÖ No significant memory leaks detected')
    }

    return {
      initialMemory,
      finalMemory,
      memoryIncrease,
      leakDetected,
    }
  }
}

// CLI runner for standalone execution
if (typeof require !== 'undefined' && require.main === module) {
  const runner = new PerformanceBenchmarkRunner()

  console.log('üöÄ Starting Bias Analysis API Performance Benchmarks...\n')

  runner
    .runFullBenchmark()
    .then((results) => {
      const report = runner.generateReport(results)
      console.log('\n' + report)

      return runner.runMemoryLeakTest()
    })
    .then((memoryResults) => {
      console.log('\nüß† Memory Leak Test Results:')
      console.log(
        `   Memory Increase: ${(memoryResults.memoryIncrease / 1024 / 1024).toFixed(2)} MB`,
      )
      console.log(
        `   Leak Detected: ${memoryResults.leakDetected ? 'YES' : 'NO'}`,
      )

      process.exit(0)
    })
    .catch((error) => {
      console.error('‚ùå Benchmark failed:', error)
      process.exit(1)
    })
}
