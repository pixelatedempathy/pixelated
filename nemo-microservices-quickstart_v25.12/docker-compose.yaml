name: nemo-microservices

configs:
  platform_config:
    content: |
      # platform is the platform-wide configuration for NeMo Microservices Platform
      platform:
        debug: false
        log_level: "INFO"
        log_format: "plain"
        base_url: "http://envoy-gateway:8000"
        jobs_url: "http://nmp-core:8000"
        models_url: "http://nmp-core:8000"
        datastore_url: "http://datastore:3000/v1/hf"
        host: "0.0.0.0"
        port: 8000

      data_designer:
        seed_dataset_source_registry:
          sources:
            - endpoint: "http://datastore:3000/v1/hf"

        model_provider_registry:
          default: "nvidiabuild"
          providers:
            - name: "nvidiabuild"
              endpoint: "https://integrate.api.nvidia.com/v1"
              api_key: "NIM_API_KEY"
      safe_synthesizer:
        classify_llm_endpoint_url: "https://integrate.api.nvidia.com/v1"
        classify_llm_model_id: "qwen/qwen2.5-coder-32b-instruct"

      jobs:
        reconcile_interval_seconds: 2
        scheduler_interval_seconds: 5

        secrets:
          backend: vault
          vault:
            address: "http://openbao:8200"
            token: "root"

        executors:
          - provider: cpu
            profile: default
            backend: docker
            config:
              storage:
                volume_name: nemo-microservices_jobs_storage
          - provider: gpu
            profile: default
            backend: docker
            config:
              storage:
                volume_name: nemo-microservices_jobs_storage

      models:
        host: "0.0.0.0"
        port: 8000
        huggingface_model_puller: "nvcr.io/nvidia/nemo-microservices/nds-v2-huggingface-cli:25.10"
        controller:
          interval_seconds: 10
          model_deployment_garbage_collection_ttl_seconds: 30
          backends:
            docker:
              enabled: true
              config: {}
        secrets:
          backend: vault
          vault:
            address: "http://openbao:8200"
            token: "root"

      inference_gateway:
        host: "0.0.0.0"
        port: 8000
        refresh_model_cache_interval_sec: 3
        secrets:
          backend: vault
          vault:
            address: "http://openbao:8200"
            token: "root"

include:
  # Platform infra
  - services/envoy.yaml
  - services/infrastructure.yaml

  # Functional microservice
  - services/data_designer.yaml
  - services/auditor.yaml
  - services/evaluator.yaml
  - services/guardrails.yaml
  - services/customizer.yaml
  - services/safe_synthesizer.yaml
  - services/intake.yaml


networks:
  nmp:
    driver: bridge
