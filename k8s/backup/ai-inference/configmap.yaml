---
# ConfigMap for AI Inference Service

apiVersion: v1
kind: ConfigMap
metadata:
  name: ai-inference-config
  namespace: pixelated-prod
  labels:
    app: pixelated
    component: ai-inference
    environment: production
data:
  # Redis Configuration
  redis-host: "redis-master.pixelated-prod.svc.cluster.local"
  redis-port: "6379"
  
  # Model Configuration
  model-name: "therapeutic_moe_model"
  model-version: "v1.0"
  
  # Performance Settings
  max-batch-size: "8"
  max-sequence-length: "2048"
  inference-timeout-ms: "2000"
  
  # Caching Settings
  enable-caching: "true"
  cache-ttl-seconds: "3600"
  cache-max-size-mb: "1024"
  
  # Load Balancing Settings
  connection-pool-size: "100"
  max-concurrent-requests: "200"
  request-timeout-ms: "5000"
  
  # Health Check Settings
  health-check-interval-seconds: "10"
  health-check-timeout-seconds: "5"
  
  # Monitoring Settings
  enable-metrics: "true"
  metrics-port: "9090"
  log-level: "INFO"
  
  # Feature Flags
  bias-detection-enabled: "true"
  progress-tracking-enabled: "true"
  session-continuity-enabled: "true"
  
  # Rate Limiting
  rate-limit-requests-per-minute: "1000"
  rate-limit-burst: "100"

---
# Prometheus Rules for AI Inference
apiVersion: v1
kind: ConfigMap
metadata:
  name: ai-inference-prometheus-rules
  namespace: pixelated-prod
  labels:
    app: pixelated
    component: monitoring
data:
  ai-inference-rules.yml: |
    groups:
    - name: ai-inference.rules
      interval: 15s
      rules:
      
      # Response Time Alerts (Critical for <2s requirement)
      - alert: SlowInferenceResponseTime
        expr: histogram_quantile(0.95, rate(inference_response_time_seconds_bucket[5m])) > 2
        for: 2m
        labels:
          severity: critical
          service: ai-inference
        annotations:
          summary: "AI inference response time exceeds 2 seconds"
          description: "95th percentile response time is {{ $value }}s (threshold: 2s)"
      
      - alert: VerySlowInferenceResponseTime
        expr: histogram_quantile(0.95, rate(inference_response_time_seconds_bucket[5m])) > 5
        for: 1m
        labels:
          severity: critical
          service: ai-inference
        annotations:
          summary: "AI inference response time critically slow"
          description: "95th percentile response time is {{ $value }}s"
      
      # High Error Rate
      - alert: HighInferenceErrorRate
        expr: rate(inference_errors_total[5m]) / rate(inference_requests_total[5m]) > 0.05
        for: 3m
        labels:
          severity: critical
          service: ai-inference
        annotations:
          summary: "High inference error rate"
          description: "Error rate is {{ $value | humanizePercentage }} (threshold: 5%)"
      
      # GPU Utilization
      - alert: LowGPUUtilization
        expr: avg(gpu_utilization) < 30
        for: 10m
        labels:
          severity: warning
          service: ai-inference
        annotations:
          summary: "Low GPU utilization"
          description: "GPU utilization is {{ $value }}% (may indicate underutilization)"
      
      - alert: HighGPUUtilization
        expr: avg(gpu_utilization) > 95
        for: 5m
        labels:
          severity: warning
          service: ai-inference
        annotations:
          summary: "High GPU utilization"
          description: "GPU utilization is {{ $value }}% (may need scaling)"
      
      # Memory Pressure
      - alert: HighMemoryUsage
        expr: container_memory_usage_bytes{pod=~"ai-inference-.*"} / container_spec_memory_limit_bytes > 0.90
        for: 5m
        labels:
          severity: warning
          service: ai-inference
        annotations:
          summary: "High memory usage"
          description: "Memory usage is {{ $value | humanizePercentage }}"
      
      # Request Queue Depth
      - alert: HighRequestQueueDepth
        expr: inference_queue_depth > 50
        for: 2m
        labels:
          severity: warning
          service: ai-inference
        annotations:
          summary: "High request queue depth"
          description: "Queue depth is {{ $value }} requests (may need scaling)"
      
      # Model Loading Issues
      - alert: ModelLoadingFailure
        expr: increase(model_loading_errors_total[5m]) > 0
        for: 1m
        labels:
          severity: critical
          service: ai-inference
        annotations:
          summary: "Model loading failure detected"
          description: "{{ $value }} model loading errors in last 5 minutes"
      
      # Cache Performance
      - alert: LowCacheHitRate
        expr: rate(cache_hits_total[5m]) / (rate(cache_hits_total[5m]) + rate(cache_misses_total[5m])) < 0.70
        for: 10m
        labels:
          severity: info
          service: ai-inference
        annotations:
          summary: "Low cache hit rate"
          description: "Cache hit rate is {{ $value | humanizePercentage }} (threshold: 70%)"
      
      # Bias Detection
      - alert: HighBiasScoreFrequency
        expr: rate(high_bias_score_total[5m]) > 0.1
        for: 5m
        labels:
          severity: warning
          service: bias-detection
        annotations:
          summary: "High frequency of high bias scores"
          description: "{{ $value }} high bias scores per second"
      
      # Database Connection
      - alert: DatabaseConnectionPoolExhausted
        expr: postgres_connection_pool_active / postgres_connection_pool_max > 0.90
        for: 2m
        labels:
          severity: warning
          service: database
        annotations:
          summary: "Database connection pool near exhaustion"
          description: "{{ $value | humanizePercentage }} of connections in use"
      
      # Pod Health
      - alert: InferencePodNotReady
        expr: kube_pod_status_ready{pod=~"ai-inference-.*", condition="true"} == 0
        for: 5m
        labels:
          severity: critical
          service: ai-inference
        annotations:
          summary: "AI inference pod not ready"
          description: "Pod {{ $labels.pod }} has been not ready for 5 minutes"
      
      # Throughput
      - alert: LowInferenceThroughput
        expr: rate(inference_requests_total[5m]) < 10
        for: 10m
        labels:
          severity: info
          service: ai-inference
        annotations:
          summary: "Low inference throughput"
          description: "Only {{ $value }} requests per second"
      
      # Progress Tracking
      - alert: ProgressTrackingDatabaseError
        expr: rate(progress_tracking_errors_total[5m]) > 1
        for: 3m
        labels:
          severity: warning
          service: progress-tracking
        annotations:
          summary: "Progress tracking database errors"
          description: "{{ $value }} errors per second"

    - name: ai-inference-slo.rules
      interval: 30s
      rules:
      
      # SLO: 99.9% availability
      - record: slo:availability:ratio
        expr: |
          sum(rate(inference_requests_total{status!~"5.."}[5m]))
          /
          sum(rate(inference_requests_total[5m]))
      
      - alert: SLOAvailabilityBreach
        expr: slo:availability:ratio < 0.999
        for: 5m
        labels:
          severity: critical
          service: ai-inference
          slo: availability
        annotations:
          summary: "SLO availability breach"
          description: "Availability is {{ $value | humanizePercentage }} (SLO: 99.9%)"
      
      # SLO: <2s response time (95th percentile)
      - record: slo:latency:p95
        expr: histogram_quantile(0.95, rate(inference_response_time_seconds_bucket[5m]))
      
      - alert: SLOLatencyBreach
        expr: slo:latency:p95 > 2
        for: 3m
        labels:
          severity: critical
          service: ai-inference
          slo: latency
        annotations:
          summary: "SLO latency breach"
          description: "P95 latency is {{ $value }}s (SLO: <2s)"
      
      # SLO: <1% error rate
      - record: slo:error_rate:ratio
        expr: |
          sum(rate(inference_errors_total[5m]))
          /
          sum(rate(inference_requests_total[5m]))
      
      - alert: SLOErrorRateBreach
        expr: slo:error_rate:ratio > 0.01
        for: 5m
        labels:
          severity: critical
          service: ai-inference
          slo: error_rate
        annotations:
          summary: "SLO error rate breach"
          description: "Error rate is {{ $value | humanizePercentage }} (SLO: <1%)"
