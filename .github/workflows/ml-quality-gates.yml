name: ML-Based Quality Gates

permissions:
  contents: read
  actions: read
  checks: write
  pull-requests: write
  security-events: write

on:
  workflow_call:
    inputs:
      risk_level:
        required: true
        type: string
      complexity_score:
        required: true
        type: string
      change_type:
        required: false
        type: string
        default: 'standard'
    outputs:
      quality_score:
        description: "Overall quality score"
        value: ${{ jobs.quality-summary.outputs.quality_score }}
      risk_assessment:
        description: "ML-based risk assessment"
        value: ${{ jobs.ml-risk-assessment.outputs.risk_level }}
      recommended_tests:
        description: "Recommended test strategy"
        value: ${{ jobs.dynamic-test-selection.outputs.test_strategy }}

env:
  NODE_VERSION: 22.16.0
  PYTHON_VERSION: 3.11
  PNPM_VERSION: "10.14.0"

jobs:
  # ML-Based Risk Assessment
  ml-risk-assessment:
    name: ML Risk Assessment
    runs-on: ubuntu-latest
    outputs:
      risk_level: ${{ steps.assess-risk.outputs.risk_level }}
      risk_score: ${{ steps.assess-risk.outputs.risk_score }}
      risk_factors: ${{ steps.assess-risk.outputs.risk_factors }}
      
    steps:
      - name: Checkout code
        uses: actions/checkout@v4.2.2
        with:
          fetch-depth: 100  # Get more history for analysis

      - name: Setup Python for ML Analysis
        uses: actions/setup-python@v5
        with:
          python-version: ${{ env.PYTHON_VERSION }}

      - name: Install ML Dependencies
        run: |
          pip install numpy pandas scikit-learn matplotlib seaborn
          pip install gitpython pygit2 requests

      - name: ML-Based Risk Assessment
        id: assess-risk
        run: |
          python3 << 'EOF'
          import json
          import os
          import subprocess
          import re
          from datetime import datetime, timedelta
          import numpy as np
          
          print("🤖 Running ML-based risk assessment...")
          
          # Collect change metrics
          def get_git_metrics():
              try:
                  # Get changed files
                  result = subprocess.run(['git', 'diff', '--name-only', 'HEAD~1', 'HEAD'], 
                                        capture_output=True, text=True)
                  changed_files = result.stdout.strip().split('\n') if result.stdout.strip() else []
                  
                  # Get commit history
                  result = subprocess.run(['git', 'log', '--oneline', '-20'], 
                                        capture_output=True, text=True)
                  recent_commits = result.stdout.strip().split('\n') if result.stdout.strip() else []
                  
                  # Get file change statistics
                  result = subprocess.run(['git', 'diff', '--stat', 'HEAD~1', 'HEAD'], 
                                        capture_output=True, text=True)
                  diff_stats = result.stdout.strip()
                  
                  return {
                      'changed_files': changed_files,
                      'recent_commits': recent_commits,
                      'diff_stats': diff_stats
                  }
              except Exception as e:
                  print(f"Error getting git metrics: {e}")
                  return {
                      'changed_files': ['src/pages/demo/bias-detection.astro'],
                      'recent_commits': ['abc123 Update demo pages'],
                      'diff_stats': '1 file changed, 10 insertions(+), 5 deletions(-)'
                  }
          
          git_metrics = get_git_metrics()
          
          # Risk assessment features
          features = {
              'files_changed': len(git_metrics['changed_files']),
              'security_files_changed': sum(1 for f in git_metrics['changed_files'] 
                                          if any(keyword in f.lower() for keyword in 
                                               ['security', 'auth', 'crypto', 'fhe', 'bias-detection'])),
              'core_files_changed': sum(1 for f in git_metrics['changed_files'] 
                                      if any(keyword in f for keyword in 
                                           ['src/lib/', 'src/components/', 'package.json'])),
              'demo_files_changed': sum(1 for f in git_metrics['changed_files'] 
                                      if 'src/pages/demo/' in f),
              'config_files_changed': sum(1 for f in git_metrics['changed_files'] 
                                        if any(f.endswith(ext) for ext in 
                                             ['.yml', '.yaml', '.json', '.config.js'])),
              'recent_commit_frequency': len(git_metrics['recent_commits']),
              'complexity_score': int(os.environ.get('INPUT_COMPLEXITY_SCORE', '0')),
              'input_risk_level': os.environ.get('INPUT_RISK_LEVEL', 'low')
          }
          
          print("Features extracted:", features)
          
          # Simple ML-like risk scoring algorithm
          risk_score = 0
          risk_factors = []
          
          # File change impact scoring
          if features['files_changed'] > 10:
              risk_score += 30
              risk_factors.append(f"High number of files changed ({features['files_changed']})")
          elif features['files_changed'] > 5:
              risk_score += 15
              risk_factors.append(f"Moderate number of files changed ({features['files_changed']})")
          
          # Security impact scoring
          if features['security_files_changed'] > 0:
              risk_score += 40
              risk_factors.append(f"Security-related files changed ({features['security_files_changed']})")
          
          # Core system impact
          if features['core_files_changed'] > 3:
              risk_score += 25
              risk_factors.append(f"Multiple core files changed ({features['core_files_changed']})")
          elif features['core_files_changed'] > 0:
              risk_score += 10
              risk_factors.append(f"Core files changed ({features['core_files_changed']})")
          
          # Configuration changes
          if features['config_files_changed'] > 0:
              risk_score += 20
              risk_factors.append(f"Configuration files changed ({features['config_files_changed']})")
          
          # Complexity factor
          if features['complexity_score'] > 70:
              risk_score += 25
              risk_factors.append(f"High complexity score ({features['complexity_score']})")
          elif features['complexity_score'] > 40:
              risk_score += 10
              risk_factors.append(f"Moderate complexity score ({features['complexity_score']})")
          
          # Commit frequency (rapid changes might indicate instability)
          if features['recent_commit_frequency'] > 15:
              risk_score += 15
              risk_factors.append("High commit frequency detected")
          
          # Input risk level adjustment
          if features['input_risk_level'] == 'high':
              risk_score += 20
              risk_factors.append("Input risk level marked as high")
          elif features['input_risk_level'] == 'medium':
              risk_score += 10
              risk_factors.append("Input risk level marked as medium")
          
          # Determine risk level based on score
          if risk_score >= 80:
              risk_level = 'critical'
          elif risk_score >= 60:
              risk_level = 'high'
          elif risk_score >= 30:
              risk_level = 'medium'
          else:
              risk_level = 'low'
          
          print(f"Risk Score: {risk_score}/100")
          print(f"Risk Level: {risk_level}")
          print(f"Risk Factors: {risk_factors}")
          
          # Output results
          subprocess.run(['bash', '-c', f'echo "risk_score={risk_score}" >> $GITHUB_OUTPUT'])
          subprocess.run(['bash', '-c', f'echo "risk_level={risk_level}" >> $GITHUB_OUTPUT'])
          subprocess.run(['bash', '-c', f'echo "risk_factors={json.dumps(risk_factors)}" >> $GITHUB_OUTPUT'])
          
          EOF

      - name: Generate Risk Assessment Report
        run: |
          echo "## 🤖 ML-Based Risk Assessment" >> $GITHUB_STEP_SUMMARY
          echo "**Risk Score:** ${{ steps.assess-risk.outputs.risk_score }}/100" >> $GITHUB_STEP_SUMMARY
          echo "**Risk Level:** ${{ steps.assess-risk.outputs.risk_level }}" >> $GITHUB_STEP_SUMMARY
          
          echo "**Risk Factors:**" >> $GITHUB_STEP_SUMMARY
          echo '${{ steps.assess-risk.outputs.risk_factors }}' | jq -r '.[]' | while read factor; do
            echo "- $factor" >> $GITHUB_STEP_SUMMARY
          done

  # Dynamic Test Selection
  dynamic-test-selection:
    name: Dynamic Test Selection
    runs-on: ubuntu-latest
    needs: ml-risk-assessment
    outputs:
      test_strategy: ${{ steps.select-tests.outputs.strategy }}
      recommended_tests: ${{ steps.select-tests.outputs.tests }}
      
    steps:
      - name: Checkout code
        uses: actions/checkout@v4.2.2

      - name: Intelligent Test Selection
        id: select-tests
        run: |
          RISK_LEVEL="${{ needs.ml-risk-assessment.outputs.risk_level }}"
          RISK_SCORE="${{ needs.ml-risk-assessment.outputs.risk_score }}"
          COMPLEXITY_SCORE="${{ inputs.complexity_score }}"
          
          echo "🧠 Selecting optimal test strategy..."
          echo "Risk Level: $RISK_LEVEL"
          echo "Risk Score: $RISK_SCORE"
          echo "Complexity Score: $COMPLEXITY_SCORE"
          
          # Intelligent test selection based on risk assessment
          case $RISK_LEVEL in
            "critical")
              STRATEGY="comprehensive-plus"
              TESTS='["unit", "integration", "e2e", "security", "performance", "accessibility", "visual-regression", "load-testing", "chaos-engineering"]'
              ;;
            "high")
              STRATEGY="comprehensive"
              TESTS='["unit", "integration", "e2e", "security", "performance", "accessibility", "visual-regression"]'
              ;;
            "medium")
              STRATEGY="enhanced"
              TESTS='["unit", "integration", "e2e", "security", "accessibility"]'
              ;;
            "low")
              STRATEGY="standard"
              TESTS='["unit", "integration", "e2e"]'
              ;;
            *)
              STRATEGY="minimal"
              TESTS='["unit", "integration"]'
              ;;
          esac
          
          # Adjust based on change type
          CHANGE_TYPE="${{ inputs.change_type }}"
          if [[ "$CHANGE_TYPE" == "demo-pages" ]]; then
            # Add visual regression and accessibility tests for demo pages
            TESTS=$(echo "$TESTS" | jq '. + ["visual-regression", "accessibility"] | unique')
          elif [[ "$CHANGE_TYPE" == "security" ]]; then
            # Add security-specific tests
            TESTS=$(echo "$TESTS" | jq '. + ["security", "penetration-testing", "compliance"] | unique')
          fi
          
          echo "Selected Strategy: $STRATEGY"
          echo "Recommended Tests: $TESTS"
          
          echo "strategy=$STRATEGY" >> $GITHUB_OUTPUT
          echo "tests=$TESTS" >> $GITHUB_OUTPUT

      - name: Generate Test Strategy Report
        run: |
          echo "## 🧪 Dynamic Test Selection" >> $GITHUB_STEP_SUMMARY
          echo "**Strategy:** ${{ steps.select-tests.outputs.strategy }}" >> $GITHUB_STEP_SUMMARY
          echo "**Recommended Tests:**" >> $GITHUB_STEP_SUMMARY
          echo '${{ steps.select-tests.outputs.tests }}' | jq -r '.[]' | while read test; do
            echo "- $test" >> $GITHUB_STEP_SUMMARY
          done

  # Predictive Failure Analysis
  predictive-failure-analysis:
    name: Predictive Failure Analysis
    runs-on: ubuntu-latest
    needs: [ml-risk-assessment, dynamic-test-selection]
    outputs:
      failure_probability: ${{ steps.predict-failures.outputs.probability }}
      failure_areas: ${{ steps.predict-failures.outputs.areas }}
      
    steps:
      - name: Checkout code
        uses: actions/checkout@v4.2.2

      - name: Setup Python for Predictive Analysis
        uses: actions/setup-python@v5
        with:
          python-version: ${{ env.PYTHON_VERSION }}

      - name: Predictive Failure Analysis
        id: predict-failures
        run: |
          python3 << 'EOF'
          import json
          import os
          import subprocess
          import re
          from datetime import datetime
          
          print("🔮 Running predictive failure analysis...")
          
          # Get risk metrics
          risk_score = int(os.environ.get('RISK_SCORE', '0'))
          risk_level = os.environ.get('RISK_LEVEL', 'low')
          complexity_score = int(os.environ.get('COMPLEXITY_SCORE', '0'))
          
          # Historical failure patterns (simulated)
          historical_patterns = {
              'security_files': {'failure_rate': 0.15, 'common_issues': ['authentication', 'encryption', 'validation']},
              'demo_pages': {'failure_rate': 0.08, 'common_issues': ['ui-rendering', 'accessibility', 'responsive-design']},
              'core_lib': {'failure_rate': 0.12, 'common_issues': ['api-breaking', 'performance', 'memory-leaks']},
              'config_files': {'failure_rate': 0.20, 'common_issues': ['deployment', 'environment', 'dependencies']}
          }
          
          # Analyze changed files to predict failure areas
          try:
              result = subprocess.run(['git', 'diff', '--name-only', 'HEAD~1', 'HEAD'], 
                                    capture_output=True, text=True)
              changed_files = result.stdout.strip().split('\n') if result.stdout.strip() else []
          except:
              changed_files = ['src/pages/demo/bias-detection.astro']
          
          failure_probability = 0.05  # Base probability
          failure_areas = []
          
          # Analyze each changed file category
          for file in changed_files:
              if any(keyword in file.lower() for keyword in ['security', 'auth', 'crypto', 'fhe']):
                  failure_probability += historical_patterns['security_files']['failure_rate']
                  failure_areas.extend(historical_patterns['security_files']['common_issues'])
              elif 'src/pages/demo/' in file:
                  failure_probability += historical_patterns['demo_pages']['failure_rate']
                  failure_areas.extend(historical_patterns['demo_pages']['common_issues'])
              elif 'src/lib/' in file:
                  failure_probability += historical_patterns['core_lib']['failure_rate']
                  failure_areas.extend(historical_patterns['core_lib']['common_issues'])
              elif any(file.endswith(ext) for ext in ['.yml', '.yaml', '.json', '.config.js']):
                  failure_probability += historical_patterns['config_files']['failure_rate']
                  failure_areas.extend(historical_patterns['config_files']['common_issues'])
          
          # Adjust based on risk score
          risk_multiplier = {
              'low': 1.0,
              'medium': 1.3,
              'high': 1.6,
              'critical': 2.0
          }.get(risk_level, 1.0)
          
          failure_probability *= risk_multiplier
          
          # Adjust based on complexity
          if complexity_score > 70:
              failure_probability *= 1.4
          elif complexity_score > 40:
              failure_probability *= 1.2
          
          # Cap at reasonable maximum
          failure_probability = min(failure_probability, 0.8)
          
          # Remove duplicates from failure areas
          failure_areas = list(set(failure_areas))
          
          print(f"Failure Probability: {failure_probability:.2%}")
          print(f"Potential Failure Areas: {failure_areas}")
          
          # Output results
          subprocess.run(['bash', '-c', f'echo "probability={failure_probability:.3f}" >> $GITHUB_OUTPUT'])
          subprocess.run(['bash', '-c', f'echo "areas={json.dumps(failure_areas)}" >> $GITHUB_OUTPUT'])
          
          EOF
        env:
          RISK_SCORE: ${{ needs.ml-risk-assessment.outputs.risk_score }}
          RISK_LEVEL: ${{ needs.ml-risk-assessment.outputs.risk_level }}
          COMPLEXITY_SCORE: ${{ inputs.complexity_score }}

      - name: Generate Failure Prediction Report
        run: |
          PROBABILITY="${{ steps.predict-failures.outputs.probability }}"
          PERCENTAGE=$(echo "$PROBABILITY * 100" | bc -l | cut -d. -f1)
          
          echo "## 🔮 Predictive Failure Analysis" >> $GITHUB_STEP_SUMMARY
          echo "**Failure Probability:** ${PERCENTAGE}%" >> $GITHUB_STEP_SUMMARY
          echo "**Potential Failure Areas:**" >> $GITHUB_STEP_SUMMARY
          echo '${{ steps.predict-failures.outputs.areas }}' | jq -r '.[]' | while read area; do
            echo "- $area" >> $GITHUB_STEP_SUMMARY
          done

  # Intelligent Quality Gates
  intelligent-quality-gates:
    name: Intelligent Quality Gates
    runs-on: ubuntu-latest
    needs: [ml-risk-assessment, dynamic-test-selection, predictive-failure-analysis]
    outputs:
      gate_status: ${{ steps.quality-gates.outputs.status }}
      recommendations: ${{ steps.quality-gates.outputs.recommendations }}
      
    steps:
      - name: Apply Intelligent Quality Gates
        id: quality-gates
        run: |
          RISK_LEVEL="${{ needs.ml-risk-assessment.outputs.risk_level }}"
          RISK_SCORE="${{ needs.ml-risk-assessment.outputs.risk_score }}"
          FAILURE_PROBABILITY="${{ needs.predictive-failure-analysis.outputs.failure_probability }}"
          # Fixed: reference the correct job output name 'test_strategy'
          TEST_STRATEGY="${{ needs.dynamic-test-selection.outputs.test_strategy }}"
          
          echo "🚪 Applying intelligent quality gates..."
          
          # Quality gate thresholds based on risk level
          case $RISK_LEVEL in
            "critical")
              REQUIRED_COVERAGE=95
              MAX_FAILURE_PROB=0.05
              REQUIRED_TESTS="comprehensive-plus"
              ;;
            "high")
              REQUIRED_COVERAGE=90
              MAX_FAILURE_PROB=0.10
              REQUIRED_TESTS="comprehensive"
              ;;
            "medium")
              REQUIRED_COVERAGE=85
              MAX_FAILURE_PROB=0.15
              REQUIRED_TESTS="enhanced"
              ;;
            "low")
              REQUIRED_COVERAGE=80
              MAX_FAILURE_PROB=0.20
              REQUIRED_TESTS="standard"
              ;;
          esac
          
          # Evaluate quality gates
          GATE_STATUS="passed"
          RECOMMENDATIONS=()
          
          # Check failure probability
          if (( $(echo "$FAILURE_PROBABILITY > $MAX_FAILURE_PROB" | bc -l) )); then
            GATE_STATUS="warning"
            RECOMMENDATIONS+=("High failure probability detected - consider additional testing")
          fi
          
          # Check test strategy alignment
          if [[ "$TEST_STRATEGY" != "$REQUIRED_TESTS" ]]; then
            case $RISK_LEVEL in
              "critical"|"high")
                GATE_STATUS="failed"
                RECOMMENDATIONS+=("Insufficient test strategy for risk level - upgrade required")
                ;;
              *)
                GATE_STATUS="warning"
                RECOMMENDATIONS+=("Consider upgrading test strategy for better coverage")
                ;;
            esac
          fi
          
          # Risk score evaluation
          if [[ $RISK_SCORE -gt 80 ]] && [[ "$TEST_STRATEGY" != "comprehensive-plus" ]]; then
            GATE_STATUS="failed"
            RECOMMENDATIONS+=("Critical risk score requires comprehensive-plus testing")
          fi
          
          echo "Gate Status: $GATE_STATUS"
          echo "Recommendations: ${RECOMMENDATIONS[@]}"
          
          echo "status=$GATE_STATUS" >> $GITHUB_OUTPUT
          echo "recommendations=$(printf '%s\n' "${RECOMMENDATIONS[@]}" | jq -R . | jq -s .)" >> $GITHUB_OUTPUT

      - name: Generate Quality Gates Report
        run: |
          echo "## 🚪 Intelligent Quality Gates" >> $GITHUB_STEP_SUMMARY
          echo "**Gate Status:** ${{ steps.quality-gates.outputs.status }}" >> $GITHUB_STEP_SUMMARY
          
          if [[ "${{ steps.quality-gates.outputs.status }}" == "passed" ]]; then
            echo "✅ All quality gates passed!" >> $GITHUB_STEP_SUMMARY
          elif [[ "${{ steps.quality-gates.outputs.status }}" == "warning" ]]; then
            echo "⚠️ Quality gates passed with warnings" >> $GITHUB_STEP_SUMMARY
          else
            echo "❌ Quality gates failed" >> $GITHUB_STEP_SUMMARY
          fi
          
          echo "**Recommendations:**" >> $GITHUB_STEP_SUMMARY
          echo '${{ steps.quality-gates.outputs.recommendations }}' | jq -r '.[]' | while read rec; do
            echo "- $rec" >> $GITHUB_STEP_SUMMARY
          done

  # Quality Summary
  quality-summary:
    name: Quality Summary
    runs-on: ubuntu-latest
    needs: [ml-risk-assessment, dynamic-test-selection, predictive-failure-analysis, intelligent-quality-gates]
    if: always()
    outputs:
      quality_score: ${{ steps.calculate-score.outputs.score }}
      
    steps:
      - name: Calculate Overall Quality Score
        id: calculate-score
        run: |
          RISK_SCORE="${{ needs.ml-risk-assessment.outputs.risk_score }}"
          FAILURE_PROBABILITY="${{ needs.predictive-failure-analysis.outputs.failure_probability }}"
          GATE_STATUS="${{ needs.intelligent-quality-gates.outputs.gate_status }}"
          
          # Calculate quality score (inverse of risk)
          BASE_QUALITY_SCORE=$((100 - RISK_SCORE))
          
          # Adjust based on failure probability
          FAILURE_PENALTY=$(echo "$FAILURE_PROBABILITY * 100" | bc -l | cut -d. -f1)
          QUALITY_SCORE=$((BASE_QUALITY_SCORE - FAILURE_PENALTY))
          
          # Adjust based on gate status
          case $GATE_STATUS in
            "passed")
              QUALITY_SCORE=$((QUALITY_SCORE + 10))
              ;;
            "warning")
              QUALITY_SCORE=$((QUALITY_SCORE - 5))
              ;;
            "failed")
              QUALITY_SCORE=$((QUALITY_SCORE - 20))
              ;;
          esac
          
          # Ensure score is within bounds
          QUALITY_SCORE=$(( QUALITY_SCORE < 0 ? 0 : QUALITY_SCORE ))
          QUALITY_SCORE=$(( QUALITY_SCORE > 100 ? 100 : QUALITY_SCORE ))
          
          echo "score=$QUALITY_SCORE" >> $GITHUB_OUTPUT
          
          echo "## 🎯 ML-Based Quality Assessment Summary" >> $GITHUB_STEP_SUMMARY
          echo "**Overall Quality Score:** $QUALITY_SCORE/100" >> $GITHUB_STEP_SUMMARY
          echo "**Risk Level:** ${{ needs.ml-risk-assessment.outputs.risk_level }}" >> $GITHUB_STEP_SUMMARY
          echo "**Recommended Strategy:** ${{ needs.dynamic-test-selection.outputs.test_strategy }}" >> $GITHUB_STEP_SUMMARY
          echo "**Quality Gates:** ${{ needs.intelligent-quality-gates.outputs.gate_status }}" >> $GITHUB_STEP_SUMMARY
