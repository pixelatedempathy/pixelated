"""
Flask Service Integration for 6-Stage Pipeline Orchestration

Provides seamless integration between the Flask-based 6-stage pipeline system
and the MCP server's WebSocket real-time communication infrastructure.
"""

import asyncio
import json
import logging
from datetime import datetime
from typing import Dict, Any, Optional, List
from dataclasses import dataclass
from enum import Enum

import aiohttp
import structlog
from redis.asyncio import Redis

from ..config import MCPConfig
from ..exceptions import IntegrationError, ValidationError, ServiceUnavailableError
from .integration_manager import IntegrationEventType

logger = structlog.get_logger(__name__)


class PipelineStage(Enum):
    """6-stage pipeline stages."""
    DATA_INGESTION = "data_ingestion"
    PREPROCESSING = "preprocessing"
    BIAS_DETECTION = "bias_detection"
    STANDARDIZATION = "standardization"
    VALIDATION = "validation"
    OUTPUT_GENERATION = "output_generation"


class PipelineStatus(Enum):
    """Pipeline execution status."""
    PENDING = "pending"
    RUNNING = "running"
    COMPLETED = "completed"
    FAILED = "failed"
    CANCELLED = "cancelled"


@dataclass
class PipelineExecution:
    """Pipeline execution data structure."""
    execution_id: str
    user_id: str
    pipeline_config: Dict[str, Any]
    status: PipelineStatus
    current_stage: Optional[PipelineStage]
    stage_results: Dict[str, Any]
    start_time: datetime
    end_time: Optional[datetime]
    overall_progress: float
    quality_score: Optional[float]
    error_message: Optional[str]


class FlaskIntegrationService:
    """
    Integration service for Flask-based 6-stage pipeline orchestration.
    
    Handles communication with the Flask service, manages pipeline execution,
    and provides real-time updates via WebSocket integration.
    """
    
    def __init__(self, config: MCPConfig, redis_client: Redis, integration_manager=None):
        """
        Initialize Flask integration service.
        
        Args:
            config: MCP configuration
            redis_client: Redis client instance
            integration_manager: Integration manager instance
        """
        self.config = config
        self.redis_client = redis_client
        self.integration_manager = integration_manager
        self.logger = structlog.get_logger(__name__)
        
        # Flask service configuration
        self.flask_api_url = config.external_services.flask_api_url
        self.request_timeout = 300  # 5 minutes
        self.session = None
        
        # Pipeline tracking
        self.active_executions: Dict[str, PipelineExecution] = {}
        self.execution_timeout = 3600  # 1 hour
        
        self.logger.info("FlaskIntegrationService initialized", flask_api_url=self.flask_api_url)
    
    async def initialize(self) -> None:
        """Initialize service and create aiohttp session."""
        try:
            self.session = aiohttp.ClientSession(
                timeout=aiohttp.ClientTimeout(total=self.request_timeout),
                headers={'Content-Type': 'application/json'}
            )
            self.logger.info("Flask integration service initialized successfully")
            
        except Exception as e:
            self.logger.error("Failed to initialize Flask integration service", error=str(e))
            raise IntegrationError(f"Flask integration service initialization failed: {str(e)}")
    
    async def execute_pipeline(self, pipeline_config: Dict[str, Any], user_id: str) -> str:
        """
        Execute 6-stage pipeline via Flask service.
        
        Args:
            pipeline_config: Pipeline configuration
            user_id: User ID for authorization
            
        Returns:
            Execution ID
            
        Raises:
            IntegrationError: If pipeline execution fails
            ValidationError: If configuration is invalid
        """
        try:
            # Validate pipeline configuration
            self._validate_pipeline_config(pipeline_config)
            
            # Generate execution ID
            execution_id = f"pipeline_{int(datetime.utcnow().timestamp())}_{user_id}"
            
            # Create execution record
            execution = PipelineExecution(
                execution_id=execution_id,
                user_id=user_id,
                pipeline_config=pipeline_config,
                status=PipelineStatus.PENDING,
                current_stage=None,
                stage_results={},
                start_time=datetime.utcnow(),
                end_time=None,
                overall_progress=0.0,
                quality_score=None,
                error_message=None
            )
            
            self.active_executions[execution_id] = execution
            
            # Start pipeline execution
            asyncio.create_task(self._execute_pipeline_async(execution_id, pipeline_config, user_id))
            
            self.logger.info(
                "Pipeline execution initiated",
                execution_id=execution_id,
                user_id=user_id,
                pipeline_config=pipeline_config
            )
            
            return execution_id
            
        except ValidationError:
            raise
        except Exception as e:
            self.logger.error("Error initiating pipeline execution", error=str(e))
            raise IntegrationError(f"Pipeline execution initiation failed: {str(e)}")
    
    async def _execute_pipeline_async(self, execution_id: str, pipeline_config: Dict[str, Any], user_id: str) -> None:
        """Execute pipeline asynchronously with real-time updates."""
        try:
            execution = self.active_executions[execution_id]
            
            # Publish pipeline start event
            await self._publish_pipeline_event(
                IntegrationEventType.PIPELINE_STAGE_START,
                execution_id,
                user_id,
                {
                    'stage_name': 'initialization',
                    'stage_number': 0,
                    'message': 'Pipeline execution started'
                }
            )
            
            # Execute each stage of the 6-stage pipeline
            stages = [
                (PipelineStage.DATA_INGESTION, self._execute_data_ingestion),
                (PipelineStage.PREPROCESSING, self._execute_preprocessing),
                (PipelineStage.BIAS_DETECTION, self._execute_bias_detection),
                (PipelineStage.STANDARDIZATION, self._execute_standardization),
                (PipelineStage.VALIDATION, self._execute_validation),
                (PipelineStage.OUTPUT_GENERATION, self._execute_output_generation)
            ]
            
            for i, (stage, executor) in enumerate(stages, 1):
                try:
                    # Update current stage
                    execution.current_stage = stage
                    execution.overall_progress = (i - 1) * 100 / len(stages)
                    
                    # Publish stage start event
                    await self._publish_pipeline_event(
                        IntegrationEventType.PIPELINE_STAGE_START,
                        execution_id,
                        user_id,
                        {
                            'stage_name': stage.value,
                            'stage_number': i,
                            'overall_progress': execution.overall_progress
                        }
                    )
                    
                    # Execute stage
                    stage_result = await executor(pipeline_config, user_id)
                    
                    # Store stage result
                    execution.stage_results[stage.value] = stage_result
                    
                    # Update progress
                    execution.overall_progress = i * 100 / len(stages)
                    
                    # Publish stage complete event
                    await self._publish_pipeline_event(
                        IntegrationEventType.PIPELINE_STAGE_COMPLETE,
                        execution_id,
                        user_id,
                        {
                            'stage_name': stage.value,
                            'stage_number': i,
                            'result': stage_result,
                            'overall_progress': execution.overall_progress
                        }
                    )
                    
                    # Check for stage failures
                    if stage_result.get('status') == 'failed':
                        raise IntegrationError(f"Stage {stage.value} failed: {stage_result.get('error', 'Unknown error')}")
                    
                except Exception as e:
                    self.logger.error(f"Error in pipeline stage {stage.value}", execution_id=execution_id, error=str(e))
                    execution.status = PipelineStatus.FAILED
                    execution.error_message = str(e)
                    
                    # Publish error event
                    await self._publish_pipeline_event(
                        IntegrationEventType.PIPELINE_ERROR,
                        execution_id,
                        user_id,
                        {
                            'stage_name': stage.value,
                            'error': str(e),
                            'error_type': type(e).__name__
                        }
                    )
                    return
            
            # Pipeline completed successfully
            execution.status = PipelineStatus.COMPLETED
            execution.end_time = datetime.utcnow()
            execution.overall_progress = 100.0
            
            # Calculate overall quality score
            execution.quality_score = self._calculate_overall_quality_score(execution.stage_results)
            
            # Publish completion event
            await self._publish_pipeline_event(
                IntegrationEventType.PIPELINE_COMPLETE,
                execution_id,
                user_id,
                {
                    'status': 'completed',
                    'results': execution.stage_results,
                    'overall_quality_score': execution.quality_score,
                    'total_duration_seconds': (execution.end_time - execution.start_time).total_seconds()
                }
            )
            
            self.logger.info(
                "Pipeline execution completed successfully",
                execution_id=execution_id,
                quality_score=execution.quality_score
            )
            
        except Exception as e:
            self.logger.error("Pipeline execution failed", execution_id=execution_id, error=str(e))
            execution.status = PipelineStatus.FAILED
            execution.error_message = str(e)
            
            # Publish error event
            await self._publish_pipeline_event(
                IntegrationEventType.PIPELINE_ERROR,
                execution_id,
                user_id,
                {
                    'error': str(e),
                    'error_type': type(e).__name__
                }
            )
    
    async def _execute_data_ingestion(self, pipeline_config: Dict[str, Any], user_id: str) -> Dict[str, Any]:
        """Execute Stage 1: Data Ingestion."""
        try:
            self.logger.info("Executing data ingestion stage", user_id=user_id)
            
            # Simulate data ingestion via Flask API
            async with self.session.post(
                f"{self.flask_api_url}/api/v1/pipeline/stages/execute",
                json={
                    'stage': 'data_ingestion',
                    'input_data': pipeline_config.get('input_data', {}),
                    'config': pipeline_config
                }
            ) as response:
                if response.status != 200:
                    raise IntegrationError(f"Data ingestion failed: HTTP {response.status}")
                
                result = await response.json()
                
                return {
                    'stage': 'data_ingestion',
                    'status': 'completed',
                    'result': result.get('data', {}),
                    'duration_seconds': result.get('duration_seconds', 0),
                    'processed_records': result.get('data', {}).get('record_count', 0)
                }
                
        except Exception as e:
            self.logger.error("Data ingestion stage failed", error=str(e))
            return {
                'stage': 'data_ingestion',
                'status': 'failed',
                'error': str(e)
            }
    
    async def _execute_preprocessing(self, pipeline_config: Dict[str, Any], user_id: str) -> Dict[str, Any]:
        """Execute Stage 2: Preprocessing."""
        try:
            self.logger.info("Executing preprocessing stage", user_id=user_id)
            
            # Simulate preprocessing operations
            await asyncio.sleep(2)  # Simulate processing time
            
            return {
                'stage': 'preprocessing',
                'status': 'completed',
                'operations_applied': [
                    'data_type_conversion',
                    'missing_value_handling',
                    'duplicate_removal',
                    'format_standardization'
                ],
                'quality_metrics': {
                    'completeness_score': 0.98,
                    'consistency_score': 0.95,
                    'accuracy_score': 0.97
                },
                'duration_seconds': 2.0
            }
            
        except Exception as e:
            self.logger.error("Preprocessing stage failed", error=str(e))
            return {
                'stage': 'preprocessing',
                'status': 'failed',
                'error': str(e)
            }
    
    async def _execute_bias_detection(self, pipeline_config: Dict[str, Any], user_id: str) -> Dict[str, Any]:
        """Execute Stage 3: Bias Detection."""
        try:
            self.logger.info("Executing bias detection stage", user_id=user_id)
            
            # Call bias detection service if available
            if hasattr(self, 'bias_detection_service') and self.bias_detection_service:
                bias_result = await self.bias_detection_service.analyze_dataset(
                    pipeline_config.get('input_data', {}),
                    user_id
                )
            else:
                # Simulate bias detection result
                bias_result = {
                    'bias_score': 0.15,  # Low bias
                    'bias_categories': {
                        'demographic': 0.10,
                        'geographic': 0.05,
                        'temporal': 0.20
                    },
                    'recommendations': [
                        'Consider increasing dataset diversity',
                        'Review temporal bias in data collection'
                    ],
                    'compliance_status': 'acceptable',
                    'threshold_exceeded': False
                }
            
            return {
                'stage': 'bias_detection',
                'status': 'completed',
                'result': bias_result,
                'duration_seconds': 3.0
            }
            
        except Exception as e:
            self.logger.error("Bias detection stage failed", error=str(e))
            return {
                'stage': 'bias_detection',
                'status': 'failed',
                'error': str(e)
            }
    
    async def _execute_standardization(self, pipeline_config: Dict[str, Any], user_id: str) -> Dict[str, Any]:
        """Execute Stage 4: Standardization."""
        try:
            self.logger.info("Executing standardization stage", user_id=user_id)
            
            await asyncio.sleep(1)  # Simulate processing time
            
            return {
                'stage': 'standardization',
                'status': 'completed',
                'target_format': 'standardized_json',
                'schema_applied': 'dataset_schema_v2.1',
                'fields_standardized': 25,
                'data_types_normalized': 8,
                'quality_score': 0.96,
                'duration_seconds': 1.0
            }
            
        except Exception as e:
            self.logger.error("Standardization stage failed", error=str(e))
            return {
                'stage': 'standardization',
                'status': 'failed',
                'error': str(e)
            }
    
    async def _execute_validation(self, pipeline_config: Dict[str, Any], user_id: str) -> Dict[str, Any]:
        """Execute Stage 5: Validation."""
        try:
            self.logger.info("Executing validation stage", user_id=user_id)
            
            await asyncio.sleep(2)  # Simulate validation time
            
            return {
                'stage': 'validation',
                'status': 'completed',
                'validation_checks': [
                    'schema_validation',
                    'data_type_validation',
                    'completeness_check',
                    'consistency_check',
                    'business_rule_validation'
                ],
                'checks_passed': 5,
                'checks_failed': 0,
                'validation_score': 1.0,
                'quality_score': 0.98,
                'duration_seconds': 2.0
            }
            
        except Exception as e:
            self.logger.error("Validation stage failed", error=str(e))
            return {
                'stage': 'validation',
                'status': 'failed',
                'error': str(e)
            }
    
    async def _execute_output_generation(self, pipeline_config: Dict[str, Any], user_id: str) -> Dict[str, Any]:
        """Execute Stage 6: Output Generation."""
        try:
            self.logger.info("Executing output generation stage", user_id=user_id)
            
            execution_id = f"output_{int(datetime.utcnow().timestamp())}"
            
            return {
                'stage': 'output_generation',
                'status': 'completed',
                'output_files': [
                    {
                        'filename': f'processed_dataset_{execution_id}.json',
                        'format': 'json',
                        'size_mb': 2.1,
                        'record_count': 995
                    },
                    {
                        'filename': f'validation_report_{execution_id}.json',
                        'format': 'json',
                        'size_mb': 0.1,
                        'record_count': 1
                    }
                ],
                'metadata': {
                    'processing_timestamp': datetime.utcnow().isoformat(),
                    'pipeline_version': '1.0.0',
                    'bias_detection_applied': True,
                    'quality_score': 0.96
                },
                'download_links': [
                    f'/api/v1/pipeline/executions/{execution_id}/output/processed_dataset',
                    f'/api/v1/pipeline/executions/{execution_id}/output/validation_report'
                ],
                'duration_seconds': 1.5
            }
            
        except Exception as e:
            self.logger.error("Output generation stage failed", error=str(e))
            return {
                'stage': 'output_generation',
                'status': 'failed',
                'error': str(e)
            }
    
    def _validate_pipeline_config(self, config: Dict[str, Any]) -> None:
        """Validate pipeline configuration."""
        required_fields = ['source_format', 'target_format', 'input_data']
        missing_fields = [field for field in required_fields if field not in config]
        
        if missing_fields:
            raise ValidationError(f"Missing required pipeline configuration fields: {missing_fields}")
        
        supported_formats = ['csv', 'json', 'jsonl', 'parquet', 'txt']
        if config['source_format'] not in supported_formats:
            raise ValidationError(f"Unsupported source format: {config['source_format']}")
        
        if config['target_format'] not in supported_formats:
            raise ValidationError(f"Unsupported target format: {config['target_format']}")
    
    def _calculate_overall_quality_score(self, stage_results: Dict[str, Any]) -> float:
        """Calculate overall quality score from stage results."""
        quality_scores = []
        
        for stage_name, result in stage_results.items():
            if result.get('status') == 'completed':
                stage_score = result.get('quality_score') or result.get('validation_score') or 0.0
                if isinstance(stage_score, (int, float)):
                    quality_scores.append(stage_score)
        
        return sum(quality_scores) / len(quality_scores) if quality_scores else 0.0
    
    async def _publish_pipeline_event(self, event_type: IntegrationEventType, 
                                    execution_id: str, user_id: str, data: Dict[str, Any]) -> None:
        """Publish pipeline event via integration manager."""
        if self.integration_manager:
            await self.integration_manager.publish_integration_event(
                event_type,
                pipeline_id=execution_id,
                user_id=user_id,
                data=data
            )
    
    async def get_execution_status(self, execution_id: str, user_id: str) -> Dict[str, Any]:
        """
        Get pipeline execution status.
        
        Args:
            execution_id: Execution ID
            user_id: User ID for authorization
            
        Returns:
            Execution status information
        """
        try:
            execution = self.active_executions.get(execution_id)
            
            if not execution:
                raise ValidationError(f"Execution {execution_id} not found")
            
            if execution.user_id != user_id:
                raise ValidationError("Access denied to execution")
            
            return {
                'execution_id': execution_id,
                'status': execution.status.value,
                'current_stage': execution.current_stage.value if execution.current_stage else None,
                'overall_progress': execution.overall_progress,
                'quality_score': execution.quality_score,
                'stage_results': execution.stage_results,
                'start_time': execution.start_time.isoformat(),
                'end_time': execution.end_time.isoformat() if execution.end_time else None,
                'error_message': execution.error_message
            }
            
        except ValidationError:
            raise
        except Exception as e:
            self.logger.error("Error getting execution status", execution_id=execution_id, error=str(e))
            raise IntegrationError(f"Failed to get execution status: {str(e)}")
    
    async def cancel_execution(self, execution_id: str, user_id: str) -> bool:
        """
        Cancel pipeline execution.
        
        Args:
            execution_id: Execution ID
            user_id: User ID for authorization
            
        Returns:
            True if cancellation was successful
        """
        try:
            execution = self.active_executions.get(execution_id)
            
            if not execution:
                raise ValidationError(f"Execution {execution_id} not found")
            
            if execution.user_id != user_id:
                raise ValidationError("Access denied to execution")
            
            if execution.status in [PipelineStatus.COMPLETED, PipelineStatus.FAILED, PipelineStatus.CANCELLED]:
                raise ValidationError(f"Execution cannot be cancelled in {execution.status.value} status")
            
            # Update execution status
            execution.status = PipelineStatus.CANCELLED
            execution.end_time = datetime.utcnow()
            
            # Publish cancellation event
            await self._publish_pipeline_event(
                IntegrationEventType.PIPELINE_ERROR,
                execution_id,
                user_id,
                {
                    'status': 'cancelled',
                    'reason': 'User requested cancellation',
                    'cancelled_at': datetime.utcnow().isoformat()
                }
            )
            
            self.logger.info("Pipeline execution cancelled", execution_id=execution_id, user_id=user_id)
            
            return True
            
        except ValidationError:
            raise
        except Exception as e:
            self.logger.error("Error cancelling execution", execution_id=execution_id, error=str(e))
            raise IntegrationError(f"Failed to cancel execution: {str(e)}")
    
    async def get_service_health(self) -> Dict[str, Any]:
        """Get Flask service health status."""
        try:
            if not self.session:
                return {
                    'status': 'unhealthy',
                    'error': 'Service not initialized',
                    'timestamp': datetime.utcnow().isoformat()
                }
            
            # Test Flask service connectivity
            async with self.session.get(f"{self.flask_api_url}/health") as response:
                if response.status == 200:
                    return {
                        'status': 'healthy',
                        'flask_api_url': self.flask_api_url,
                        'active_executions': len(self.active_executions),
                        'timestamp': datetime.utcnow().isoformat()
                    }
                else:
                    return {
                        'status': 'unhealthy',
                        'error': f"Flask service returned HTTP {response.status}",
                        'timestamp': datetime.utcnow().isoformat()
                    }
                    
        except Exception as e:
            self.logger.error("Flask service health check failed", error=str(e))
            return {
                'status': 'unhealthy',
                'error': str(e),
                'timestamp': datetime.utcnow().isoformat()
            }
    
    async def cleanup_expired_executions(self) -> int:
        """Clean up expired pipeline executions."""
        try:
            current_time = datetime.utcnow()
            expired_count = 0
            
            expired_executions = []
            for execution_id, execution in self.active_executions.items():
                if (current_time - execution.start_time).total_seconds() > self.execution_timeout:
                    expired_executions.append(execution_id)
            
            for execution_id in expired_executions:
                del self.active_executions[execution_id]
                expired_count += 1
            
            if expired_count > 0:
                self.logger.info("Cleaned up expired executions", count=expired_count)
            
            return expired_count
            
        except Exception as e:
            self.logger.error("Error cleaning up expired executions", error=str(e))
            return 0
    
    async def shutdown(self) -> None:
        """Shutdown Flask integration service."""
        try:
            self.logger.info("Shutting down Flask integration service")
            
            # Close aiohttp session
            if self.session:
                await self.session.close()
            
            # Clear active executions
            self.active_executions.clear()
            
            self.logger.info("Flask integration service shutdown completed")
            
        except Exception as e:
            self.logger.error("Error during Flask integration service shutdown", error=str(e))