# S3 Sync Status & Manifest

**Last Updated**: 2025-12-11  
**Status**: Raw tier sync IN PROGRESS

## Current Sync Status

### From `.notes/markdown/one.md`:

- **processed tier**: ✅ DONE
- **raw tier**: ⏳ IN PROGRESS
- **Method**: `rclone` in detached tmux session
- **Command**: `rclone copy gdrive:datasets ovh:pixel-data/datasets/gdrive/raw ...`
- **Log File**: `upload_raw_final.log`
- **Reason**: Python-based uploads caused OOM/crashes. rclone is stable solution.

### Bucket Names

**Important**: There are two bucket names in use:
- **Sync target**: `pixel-data` (per one.md and check_uploads.py)
- **Training loader default**: `pixel-data` (per S3DatasetLoader)

**Current Status**: Scripts can check both buckets. The sync is targeting `pixel-data`, but training scripts may need to use `pixel-data` or the bucket name can be configured.

---

## How to Check Sync Status

### Quick Status Check
```bash
cd /home/vivi/pixelated
uv run python ai/training_ready/scripts/check_sync_status.py
```

This shows:
- What's in processed/ (canonical)
- What's in raw/ (backup, sync in progress)
- Summary of sync status

**Note**: Use `uv run` to ensure boto3 and dependencies are available.

### Generate Full Manifest
```bash
cd /home/vivi/pixelated
uv run python ai/training_ready/scripts/generate_s3_manifest.py
```

This creates:
- `ai/training_ready/data/s3_manifest.json` - Complete inventory
- Shows all objects categorized by structure
- Includes sizes, counts, and file listings

### Check tmux Session
```bash
# Attach to tmux to see sync progress
tmux attach

# Or check if rclone is running
ps aux | grep rclone

# Check log file
tail -f upload_raw_final.log
```

---

## Expected Structure

Once sync completes, S3 should have:

```
s3://pixel-data/  (or pixel-data?)
├── datasets/
│   └── gdrive/
│       ├── raw/                    # Raw Google Drive mirror (backup)
│       │   └── [current Google Drive structure]
│       │
│       └── processed/              # Canonical organized structure
│           ├── cot_reasoning/
│           ├── professional_therapeutic/
│           ├── priority/
│           └── edge_cases/
```

**Note**: The actual path structure may be `datasets/gdrive/raw/` based on the sync command.

---

## When Will Sync Complete?

**Unknown** - The sync is running in background via rclone. To check:

1. **Check tmux session**:
   ```bash
   tmux attach
   # Look for rclone process and progress
   ```

2. **Check log file**:
   ```bash
   tail -f upload_raw_final.log
   # Look for completion messages or errors
   ```

3. **Check S3 directly**:
   ```bash
   uv run python ai/training_ready/scripts/generate_s3_manifest.py
   # Compare raw/ count with expected Google Drive dataset count
   ```

4. **Check rclone status**:
   ```bash
   rclone check gdrive:datasets ovh:pixel-data/datasets/gdrive/raw --size-only
   # Shows what still needs syncing
   ```

---

## Manifest Files

### Generated Manifest
- **Location**: `ai/training_ready/data/s3_manifest.json`
- **Generated by**: `scripts/generate_s3_manifest.py`
- **Contains**: Complete inventory of all S3 objects with metadata

### Dataset Registry
- **Location**: `ai/data/dataset_registry.json`
- **Contains**: Dataset catalog with S3 paths (when updated)
- **Update script**: `scripts/update_manifest_s3_paths.py`

---

## Next Steps

1. **Check sync progress**:
   ```bash
   uv run python ai/training_ready/scripts/check_sync_status.py
   ```

2. **Generate manifest**:
   ```bash
   uv run python ai/training_ready/scripts/generate_s3_manifest.py
   ```

3. **Once raw sync completes**:
   - Organize raw data into `gdrive/processed/` canonical structure
   - Update `dataset_registry.json` with S3 paths
   - Verify training scripts can access S3 datasets

4. **Monitor sync**:
   - Check tmux session periodically
   - Review log file for errors
   - Run manifest generator to track progress

---

## Related Files

- `.notes/markdown/one.md` - Original sync status
- `check_uploads.py` - Checks `pixel-data` bucket
- `ai/training_ready/scripts/check_sync_status.py` - Quick status check
- `ai/training_ready/scripts/generate_s3_manifest.py` - Full manifest generator
- `upload_raw_final.log` - Sync log file (if exists)

---

## Usage Notes

**Important**: Always use `uv run` when running these scripts:
```bash
uv run python ai/training_ready/scripts/check_sync_status.py
```

This ensures:
- boto3 and python-dotenv are available
- .env file is loaded from `ai/.env`
- OVH S3 credentials are found
