---
post_title: Priority 2 Optimization Results
author1: System
post_slug: priority-2-optimization-results
microsoft_alias: ""
featured_image: ""
categories: ["infrastructure", "kubernetes"]
tags:
  - kubernetes
  - optimization
  - resource-management
  - vpa
ai_note: "This document captures Priority 2 optimization outcomes including resource quota implementation and VPA evaluation."
summary: "Documentation of Priority 2.2 completion (resource requests/limits on all pods) and evaluation of Priority 2.1 (node size reduction) and Priority 2.3 (Vertical Pod Autoscaler), with implementation details, verification steps, and next steps."
post_date: 2025-11-09
---

## Priority 2 Optimization Results

**Date:** 2025-11-09  
**Cluster:** pixelated-empathy-civo  
**Status:** Priority 2.2 Completed, Priority 2.1 & 2.3 Evaluated

---

## ‚úÖ Priority 2.2: Set Resource Requests/Limits on All Pods - COMPLETED

### Implementation Summary

All pods now have appropriate resource requests and limits set based on actual usage patterns.

### Cert-Manager Optimization

**Before:**
- No resource requests/limits set
- Actual usage: 1-2m CPU, 28-38Mi memory per pod

**After:**
- **Requests:** 50m CPU, 64Mi memory
- **Limits:** 200m CPU, 128Mi memory
- **Pods:** 3 deployments (controller, cainjector, webhook)

**Impact:**
- Better resource predictability
- Improved scheduling
- Prevents resource contention

### Traefik Optimization

**Before:**
- No resource requests/limits set
- Actual usage: 3-4m CPU, 34-78Mi memory per pod

**After:**
- **Requests:** 100m CPU, 128Mi memory
- **Limits:** 500m CPU, 256Mi memory
- **Pods:** 2 DaemonSet pods (one per node)

**Impact:**
- Adequate headroom for traffic spikes
- Better resource allocation
- Improved stability

### OTel Collector Optimization

**Before:**
- Resources set to "0" (effectively unlimited)
- Actual usage: 11-43m CPU, 48-69Mi memory per pod

**After:**
- **Requests:** 100m CPU, 128Mi memory
- **Limits:** 500m CPU, 256Mi memory
- **Pods:** 2 DaemonSet pods (one per node)

**Impact:**
- Prevents unbounded resource usage
- Better resource predictability
- Improved cluster stability

### Resource Summary

| Component | Pods | CPU Request | Memory Request | CPU Limit | Memory Limit |
|-----------|------|-------------|----------------|-----------|--------------|
| Cert-Manager (3 deployments) | 3 | 150m total | 192Mi total | 600m total | 384Mi total |
| Traefik (DaemonSet) | 2 | 200m total | 256Mi total | 1000m total | 512Mi total |
| OTel Collector (DaemonSet) | 2 | 200m total | 256Mi total | 1000m total | 512Mi total |
| **Total** | **7** | **550m** | **704Mi** | **2600m** | **1408Mi** |

### Verification

All pods verified to have resource requests/limits:
- ‚úÖ Cert-Manager controller
- ‚úÖ Cert-Manager cainjector
- ‚úÖ Cert-Manager webhook
- ‚úÖ Traefik DaemonSet
- ‚úÖ OTel Collector DaemonSet

### Expected Benefits

1. **Better Scheduling:** Kubernetes scheduler can make informed decisions
2. **Resource Predictability:** Known resource requirements for all pods
3. **Prevent Resource Contention:** Limits prevent pods from consuming excessive resources
4. **Cost Optimization:** Better resource utilization tracking
5. **Improved Stability:** Prevents OOM kills and CPU throttling

---

## üìä Priority 2.1: Node Size Reduction - EVALUATED

### Current Cluster State

- **Nodes:** 2 nodes
- **Instance Type:** g4s.kube.medium (2 CPU, 4GB RAM per node)
- **Total Capacity:** 3.64 cores CPU, 6.14GB memory
- **Current Usage:**
  - Node 1: 96% CPU requests, 63% memory usage
  - Node 2: 65% CPU requests, 45% memory usage

### Analysis

**Current Resource Requests:**
- Total CPU requests: ~3.15 cores (87% of cluster capacity)
- Total Memory requests: ~3.6GB (59% of cluster capacity)

**Node Size Reduction Feasibility:**
- ‚ùå **Not Recommended at this time**
- Current resource requests are too high for smaller nodes
- g4s.kube.small (if available) would be 1 CPU, ~2GB RAM
- Workload would not fit on smaller nodes without significant optimization

### Recommendations

1. **Current State:** Keep g4s.kube.medium nodes
2. **Future Consideration:** After further optimization, evaluate:
   - Reducing workload resource requests
   - Optimizing DaemonSet resources
   - Consider g4s.kube.small if workload requirements decrease
3. **Testing:** If downsizing is considered:
   - Test in staging environment first
   - Ensure workload fits on smaller nodes
   - Plan for node replacement (downtime or gradual migration)

### Next Steps

1. Continue optimizing workloads (reduce resource requests where possible)
2. Monitor resource usage over time
3. Re-evaluate node size reduction after workload optimization
4. Check Civo dashboard for available instance types in your region

---

## üîç Priority 2.3: Vertical Pod Autoscaler (VPA) - EVALUATED

### Evaluation Summary

**VPA Status:** Not installed  
**Cluster Compatibility:** ‚úÖ Compatible
- Metrics server: ‚úÖ Installed
- Node count: 2 nodes (adequate)
- Kubernetes version: Compatible

### Workload Analysis

**Good VPA Candidates:**
- Cert-Manager (3 deployments) - resources set, VPA can optimize
- Flux system (5 deployments) - resources set, VPA can optimize
- Metoro exporter - resources set, VPA can optimize
- CoreDNS, metrics-server, cluster-autoscaler - resources set, VPA can optimize

**VPA Conflicts:**
- Pixelated service - Has HPA enabled (cannot use VPA simultaneously)

### VPA Installation Options

#### Method 1: Official VPA Repository (Recommended)
```bash
git clone https://github.com/kubernetes/autoscaler.git
cd autoscaler/vertical-pod-autoscaler
./hack/vpa-up.sh
```

#### Method 2: Helm (Easier Management)
```bash
helm repo add fairwinds-stable https://charts.fairwinds.com/stable
helm repo update
helm install vpa fairwinds-stable/vpa --namespace vpa-system --create-namespace
```

#### Method 3: Manual Installation
```bash
kubectl apply -f https://github.com/kubernetes/autoscaler/releases/download/vertical-pod-autoscaler-0.14.0/vpa-release.yaml
```

### VPA Usage Modes

1. **Off Mode (Recommended for Start):**
   - Provides recommendations only
   - No automatic updates
   - Safe for production
   - Review recommendations for 1-2 weeks

2. **Initial Mode:**
   - Sets resources when pod is created
   - No pod evictions
   - Safe for production
   - Apply after reviewing recommendations

3. **Auto Mode (Use with Caution):**
   - Automatically updates resources
   - Can cause pod evictions
   - May cause service disruptions
   - Only for non-production or well-tested workloads

### Risk Assessment

**Risks:**
1. **Pod Evictions:** VPA in 'Auto' mode can evict pods
2. **HPA Conflicts:** Cannot use VPA and HPA together
3. **Resource Limits:** VPA can recommend resources outside node capacity
4. **Performance Impact:** VPA components consume cluster resources

**Mitigations:**
1. Start with 'Off' mode to get recommendations
2. Review recommendations for 1-2 weeks
3. Set min/max resource bounds in resourcePolicy
4. Monitor VPA component resource usage
5. Only use 'Auto' mode for non-critical workloads

### Recommended Approach

1. **Phase 1: Installation & Evaluation (Week 1-2)**
   - Install VPA in 'Off' mode
   - Review recommendations for all workloads
   - Document baseline recommendations

2. **Phase 2: Manual Application (Week 3-4)**
   - Manually apply recommendations for non-critical workloads
   - Monitor performance and stability
   - Adjust resource requests based on VPA recommendations

3. **Phase 3: Initial Mode (Week 5-6)**
   - Enable 'Initial' mode for selected workloads
   - Monitor pod creation and resource allocation
   - Verify recommendations are accurate

4. **Phase 4: Auto Mode (Optional, Week 7+)**
   - Consider 'Auto' mode for non-production workloads
   - Monitor pod evictions and service disruptions
   - Gradually enable for production workloads if stable

### Example VPA Configuration

**VPA in 'Off' Mode (Recommendations Only):**
```yaml
apiVersion: autoscaling.k8s.io/v1
kind: VerticalPodAutoscaler
metadata:
  name: cert-manager-vpa
  namespace: cert-manager
spec:
  targetRef:
    apiVersion: apps/v1
    kind: Deployment
    name: cert-manager
  updatePolicy:
    updateMode: "Off"  # Only recommendations, no auto-updates
  resourcePolicy:
    containerPolicies:
    - containerName: cert-manager-controller
      minAllowed:
        cpu: 50m
        memory: 64Mi
      maxAllowed:
        cpu: 500m
        memory: 512Mi
```

### Next Steps

1. **Decision:** Decide if VPA is needed for your use case
2. **Installation:** Install VPA in 'Off' mode if proceeding
3. **Evaluation:** Review recommendations for 1-2 weeks
4. **Implementation:** Gradually enable VPA for selected workloads
5. **Monitoring:** Monitor VPA behavior and adjust as needed

---

## üìà Priority 2 Overall Results

### Completed Tasks

- ‚úÖ **Priority 2.2:** Set resource requests/limits for all pods
  - Cert-Manager: 3 deployments optimized
  - Traefik: DaemonSet optimized
  - OTel Collector: DaemonSet optimized

### Evaluated Tasks

- üìä **Priority 2.1:** Node size reduction evaluated
  - Not recommended at this time (high resource requests)
  - Re-evaluate after further optimization

- üîç **Priority 2.3:** VPA evaluated
  - Installation guide provided
  - Usage examples provided
  - Risk assessment completed
  - Ready for implementation if needed

### Resource Impact

**Total Resources Added:**
- CPU Requests: 550m (0.55 cores)
- Memory Requests: 704Mi (~0.69GB)
- CPU Limits: 2600m (2.6 cores)
- Memory Limits: 1408Mi (~1.38GB)

**Benefits:**
- Better resource predictability
- Improved scheduling
- Prevent resource contention
- Better cost tracking
- Improved cluster stability

### Cost Impact

- **Direct Savings:** Minimal (resources were already being used)
- **Indirect Benefits:** 
  - Better resource utilization tracking
  - Prevented future resource waste
  - Improved cluster stability
  - Better cost accountability

### Monitoring & Validation

**Metrics to Monitor:**
1. Pod resource usage vs. requests/limits
2. Pod evictions and OOM kills
3. CPU throttling events
4. Cluster resource utilization
5. Service performance and stability

**Validation Period:** 24-48 hours

### Next Steps

1. **Monitor:** Watch pod behavior for 24-48 hours
2. **Validate:** Ensure no performance degradation
3. **Document:** Update runbooks with new resource requirements
4. **Optimize:** Continue optimizing workloads based on usage patterns
5. **Consider VPA:** Evaluate VPA installation if continuous optimization is needed

---

## üìù Implementation Scripts

### Scripts Created

1. **`scripts/optimize-priority-2-resources.sh`**
   - Optimizes Cert-Manager, Traefik, and OTel Collector resources
   - Verifies resource requests/limits are set
   - Waits for rollouts to complete

2. **`scripts/evaluate-node-size-reduction.sh`**
   - Evaluates node size reduction feasibility
   - Checks available instance types
   - Analyzes resource requirements

3. **`scripts/evaluate-vpa.sh`**
   - Evaluates VPA feasibility
   - Provides installation guide
   - Shows usage examples and risk assessment

### Usage

```bash
# Optimize resources (Priority 2.2)
./scripts/optimize-priority-2-resources.sh

# Evaluate node size reduction (Priority 2.1)
./scripts/evaluate-node-size-reduction.sh

# Evaluate VPA (Priority 2.3)
./scripts/evaluate-vpa.sh
```

---

## üéØ Summary

### Priority 2.2: ‚úÖ COMPLETED
- All pods now have resource requests/limits
- Better resource predictability and scheduling
- Improved cluster stability

### Priority 2.1: üìä EVALUATED
- Node size reduction not recommended at this time
- Re-evaluate after further optimization

### Priority 2.3: üîç EVALUATED
- VPA installation guide provided
- Ready for implementation if needed
- Risk assessment completed

#### Overall Status
Priority 2.2 completed successfully. Priority 2.1 and 2.3 evaluated and ready for future implementation.

---

**Report Generated:** 2025-11-09  
**Next Review:** 2025-11-11 (48 hours after implementation)

