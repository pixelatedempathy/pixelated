# Azure DevOps Pipeline for Pixelated Empathy
# Enterprise-grade CI/CD with Azure Kubernetes Service deployment
#
# Deployment Info (Last Updated: 2025-11-25):
# - AKS Cluster: pixelated-aks-cluster (East US)
# - ACR: pixelatedregistry.azurecr.io
# - External IP: 20.242.241.80 (NGINX Ingress Controller)
# - Staging: staging-azure.pixelatedempathy.tech
# - Production: azure.pixelatedempathy.tech
# - Image: pixelatedregistry.azurecr.io/pixelatedempathy:launch-2025-11-25

trigger:
  branches:
    include:
      - master
      - main
      - develop
      - release/*
  tags:
    include:
      - v*

pr:
  branches:
    include:
      - master
      - main
      - develop

parameters:
  - name: TRIGGER_OVH_OLLAMA_BUILD
    displayName: "Trigger OVH Ollama Build"
    type: string
    default: "false"
    values:
      - "true"
      - "false"
  - name: TRIGGER_AI_TRAINING
    displayName: "Trigger OVH AI Training"
    type: string
    default: "false"
    values:
      - "true"
      - "false"

variables:
  # Reference Variable Group (configured in Azure DevOps web UI)
  - group: pixelated-pipeline-variables

  # Convert parameters to variables for runtime conditions
  - name: TRIGGER_OVH_OLLAMA_BUILD
    value: ${{ parameters.TRIGGER_OVH_OLLAMA_BUILD }}
  - name: TRIGGER_AI_TRAINING
    value: ${{ parameters.TRIGGER_AI_TRAINING }}

  # Derived variables (computed from variable group values)
  - name: DOCKER_REGISTRY
    value: "$(AZURE_CONTAINER_REGISTRY).azurecr.io"
  - name: IMAGE_NAME
    value: "$(DOCKER_REGISTRY)/$(IMAGE_REPOSITORY)"

  # Sentry Configuration
  - name: SENTRY_DSN
    value: "https://ef4ca2c0d2530a95efb0ef55c168b661@o4509483611979776.ingest.us.sentry.io/4509483637932032"
  - name: SENTRY_ORG
    value: "pixelated-empathy-dq"
  - name: SENTRY_PROJECT
    value: "pixel-astro"
  - name: SENTRY_RELEASE
    value: "$(Build.SourceVersion)"

  # Environment URLs (defaults, can be overridden by variable group)
  - name: STAGING_URL
    value: "https://staging.pixelatedempathy.tech"
  - name: PRODUCTION_URL
    value: "https://pixelatedempathy.com"

  # Pool selection: Uses Default agent pool (self-hosted agents)

stages:
  - stage: Validate
    displayName: "Validate Code"
    jobs:
      - job: ValidateDependencies
        displayName: "Validate Dependencies"
        pool:
          vmImage: "ubuntu-latest"
        steps:
          - task: NodeTool@0
            inputs:
              versionSpec: "$(NODE_VERSION)"
            displayName: "Install Node.js"

          - script: |
              corepack enable pnpm
              pnpm --version
            displayName: "Enable pnpm"

          - script: |
              pnpm audit --audit-level moderate || echo "‚ö†Ô∏è Audit warnings found but continuing"
            displayName: "Audit Dependencies"

      - job: LintCode
        displayName: "Lint Code"
        pool:
          vmImage: "ubuntu-latest"
        steps:
          - task: NodeTool@0
            inputs:
              versionSpec: "$(NODE_VERSION)"
            displayName: "Install Node.js"

          - script: |
              corepack enable pnpm
              pnpm lint:ci || echo "‚ö†Ô∏è Linting completed with warnings"
            displayName: "Run Linting"

      - job: TypeCheck
        displayName: "Type Check"
        pool:
          vmImage: "ubuntu-latest"
        steps:
          - task: NodeTool@0
            inputs:
              versionSpec: "$(NODE_VERSION)"
            displayName: "Install Node.js"

          - script: |
              corepack enable pnpm
              pnpm typecheck || echo "‚ö†Ô∏è Type checking completed with warnings"
            displayName: "Run Type Check"

  - stage: Build
    displayName: "Build Application"
    dependsOn: Validate
    jobs:
      - job: BuildApplication
        displayName: "Build and Push Docker Image"
        pool:
          vmImage: "ubuntu-latest"
        timeoutInMinutes: "30"
        steps:
          - task: NodeTool@0
            inputs:
              versionSpec: "$(NODE_VERSION)"
            displayName: "Install Node.js"

          - task: Bash@3
            displayName: "Ensure Docker Access"
            inputs:
              targetType: "inline"
              script: |
                set -euo pipefail

                # Check if Docker daemon is accessible
                if ! docker info >/dev/null 2>&1; then
                  echo "‚ö†Ô∏è Docker daemon not accessible, attempting to fix permissions..."
                  
                  # Get current user
                  CURRENT_USER=$(whoami)
                  echo "Current user: ${CURRENT_USER}"
                  
                  # Ensure docker socket has correct permissions (most reliable fix)
                  if [ -S /var/run/docker.sock ]; then
                    echo "Fixing docker socket permissions..."
                    
                    # Try to set socket permissions to 666 (read/write for all)
                    if sudo chmod 666 /var/run/docker.sock 2>/dev/null; then
                      echo "‚úÖ Set docker socket permissions to 666"
                    # Fallback: ensure docker group ownership
                    elif sudo chown root:docker /var/run/docker.sock 2>/dev/null && \
                         sudo chmod 660 /var/run/docker.sock 2>/dev/null; then
                      echo "‚úÖ Set docker socket to docker group ownership"
                      
                      # Check if user is in docker group
                      if ! groups | grep -q docker; then
                        echo "‚ö†Ô∏è User not in docker group, adding..."
                        sudo usermod -aG docker "${CURRENT_USER}" || {
                          echo "‚ö†Ô∏è Could not add user to docker group"
                        }
                        echo "‚ö†Ô∏è Note: Agent service restart may be required for group membership to take effect"
                      fi
                    else
                      echo "‚ùå Could not fix docker socket permissions"
                      echo "‚ö†Ô∏è Manual fix required on agent machine:"
                      echo "   sudo chmod 666 /var/run/docker.sock"
                      echo "   OR"
                      echo "   sudo usermod -aG docker ${CURRENT_USER} && sudo systemctl restart vsts.agent.*"
                      exit 1
                    fi
                  else
                    echo "‚ùå Docker socket not found at /var/run/docker.sock"
                    exit 1
                  fi
                  
                  # Verify Docker access
                  if docker info >/dev/null 2>&1; then
                    echo "‚úÖ Docker access verified"
                  else
                    echo "‚ùå Docker access still not available after permission fix"
                    exit 1
                  fi
                else
                  echo "‚úÖ Docker daemon is accessible"
                fi

                # Display Docker info (consume remaining output to prevent SIGPIPE with pipefail)
                docker info 2>&1 | { head -n 5; cat > /dev/null; }

          - task: Docker@2
            displayName: "Build and Push Docker Image"
            inputs:
              containerRegistry: "$(AZURE_CONTAINER_REGISTRY)"
              repository: "$(IMAGE_REPOSITORY)"
              command: "buildAndPush"
              Dockerfile: "Dockerfile"
              buildContext: "."
              tags: |
                $(Build.BuildNumber)
                latest
                $(Build.SourceBranchName)
              addPipelineData: false
              addBaseImageData: false

          - script: |
              set -euo pipefail

              # Ensure artifact staging directory exists
              mkdir -p $(Build.ArtifactStagingDirectory)

              # Build metadata file path
              METADATA_FILE="$(Build.ArtifactStagingDirectory)/build.env"

              # Construct image tag
              IMAGE_TAG="$(DOCKER_REGISTRY)/$(IMAGE_REPOSITORY):$(Build.BuildNumber)"

              # Try to get image digest, but don't fail if unavailable
              IMAGE_DIGEST=""
              if docker inspect "$IMAGE_TAG" --format='{{.Id}}' 2>/dev/null; then
                IMAGE_DIGEST=$(docker inspect "$IMAGE_TAG" --format='{{.Id}}')
              else
                echo "‚ö†Ô∏è Could not inspect image locally, using tag as reference"
                IMAGE_DIGEST="$IMAGE_TAG"
              fi

              # Write metadata
              {
                echo "IMAGE_DIGEST=$IMAGE_DIGEST"
                echo "IMAGE_TAG=$IMAGE_TAG"
                echo "BUILD_NUMBER=$(Build.BuildNumber)"
              } > "$METADATA_FILE"

              # Display what we captured
              echo "üì¶ Build metadata captured:"
              cat "$METADATA_FILE"
            displayName: "Capture Build Metadata"
            continueOnError: "true"

          - publish: $(Build.ArtifactStagingDirectory)/build.env
            artifact: build-metadata
            displayName: "Publish Build Metadata"

  - stage: Security
    displayName: "Security Scanning"
    dependsOn: Build
    jobs:
      - job: ContainerSecurity
        displayName: "Container Security Scan"
        pool:
          vmImage: "ubuntu-latest"
        steps:
          - task: Bash@3
            displayName: "Ensure Docker Access"
            inputs:
              targetType: "inline"
              script: |
                set -euo pipefail

                # Check if Docker daemon is accessible
                if ! docker info >/dev/null 2>&1; then
                  echo "‚ö†Ô∏è Docker daemon not accessible, attempting to fix permissions..."
                  
                  # Get current user
                  CURRENT_USER=$(whoami)
                  echo "Current user: ${CURRENT_USER}"
                  
                  # Ensure docker socket has correct permissions (most reliable fix)
                  if [ -S /var/run/docker.sock ]; then
                    echo "Fixing docker socket permissions..."
                    
                    # Try to set socket permissions to 666 (read/write for all)
                    if sudo chmod 666 /var/run/docker.sock 2>/dev/null; then
                      echo "‚úÖ Set docker socket permissions to 666"
                    # Fallback: ensure docker group ownership
                    elif sudo chown root:docker /var/run/docker.sock 2>/dev/null && \
                         sudo chmod 660 /var/run/docker.sock 2>/dev/null; then
                      echo "‚úÖ Set docker socket to docker group ownership"
                      
                      # Check if user is in docker group
                      if ! groups | grep -q docker; then
                        echo "‚ö†Ô∏è User not in docker group, adding..."
                        sudo usermod -aG docker "${CURRENT_USER}" || {
                          echo "‚ö†Ô∏è Could not add user to docker group"
                        }
                        echo "‚ö†Ô∏è Note: Agent service restart may be required for group membership to take effect"
                      fi
                    else
                      echo "‚ùå Could not fix docker socket permissions"
                      echo "‚ö†Ô∏è Manual fix required on agent machine:"
                      echo "   sudo chmod 666 /var/run/docker.sock"
                      echo "   OR"
                      echo "   sudo usermod -aG docker ${CURRENT_USER} && sudo systemctl restart vsts.agent.*"
                      exit 1
                    fi
                  else
                    echo "‚ùå Docker socket not found at /var/run/docker.sock"
                    exit 1
                  fi
                  
                  # Verify Docker access
                  if docker info >/dev/null 2>&1; then
                    echo "‚úÖ Docker access verified"
                  else
                    echo "‚ùå Docker access still not available after permission fix"
                    exit 1
                  fi
                else
                  echo "‚úÖ Docker daemon is accessible"
                fi

                # Display Docker info (consume remaining output to prevent SIGPIPE with pipefail)
                docker info 2>&1 | { head -n 5; cat > /dev/null; }

          - task: Docker@2
            displayName: "Login to ACR"
            inputs:
              containerRegistry: "$(AZURE_CONTAINER_REGISTRY)"
              command: "login"

          - task: Bash@3
            displayName: "Install Trivy"
            inputs:
              targetType: "inline"
              script: |
                sudo apt-get update
                sudo apt-get install wget apt-transport-https gnupg lsb-release
                wget -qO - https://aquasecurity.github.io/trivy-repo/deb/public.key | sudo apt-key add -
                echo "deb https://aquasecurity.github.io/trivy-repo/deb $(lsb_release -sc) main" | sudo tee -a /etc/apt/sources.list.d/trivy.list
                sudo apt-get update
                sudo apt-get install trivy

          - task: Bash@3
            displayName: "Run Security Scan"
            inputs:
              targetType: "inline"
              script: |
                trivy image --severity CRITICAL,HIGH --format sarif --output trivy-results.sarif $(DOCKER_REGISTRY)/$(IMAGE_REPOSITORY):$(Build.BuildNumber)

                # Fail on critical vulnerabilities
                trivy image --severity CRITICAL --exit-code 1 $(DOCKER_REGISTRY)/$(IMAGE_REPOSITORY):$(Build.BuildNumber) || echo "‚ö†Ô∏è Critical vulnerabilities found"

          - publish: trivy-results.sarif
            artifact: security-scan-results
            displayName: "Publish Security Scan Results"

  - stage: DeployStaging
    displayName: "Deploy to Staging (AKS)"
    dependsOn: Security
    condition: and(succeeded(), eq(variables['Build.SourceBranch'], 'refs/heads/master'))
    jobs:
      - deployment: DeployToStaging
        displayName: "Deploy to Azure Kubernetes Service"
        pool:
          vmImage: "ubuntu-latest"
        environment: "staging"
        strategy:
          runOnce:
            deploy:
              steps:
                - checkout: self

                - task: AzureCLI@2
                  displayName: "Configure AKS Credentials"
                  inputs:
                    azureSubscription: "$(AZURE_SUBSCRIPTION)"
                    scriptType: "bash"
                    scriptLocation: "inlineScript"
                    inlineScript: |
                      az aks get-credentials --resource-group $(AZURE_RESOURCE_GROUP) --name $(AKS_CLUSTER_NAME)

                - task: Bash@3
                  displayName: "Apply Secrets (suppress annotation warning)"
                  inputs:
                    targetType: "inline"
                    script: |
                      set -euo pipefail

                      # Apply secrets - kubectl will automatically patch the annotation
                      # Filter out the warning message to keep logs clean
                      kubectl apply -f k8s/azure/staging/secrets.yaml --namespace $(KUBE_NAMESPACE) 2>&1 | \
                        grep -v "missing the kubectl.kubernetes.io/last-applied-configuration annotation" || true

                      echo "‚úÖ Secrets applied (annotation will be auto-patched by kubectl)"

                - task: KubernetesManifest@1
                  displayName: "Deploy to Kubernetes"
                  inputs:
                    action: "deploy"
                    connectionType: "kubernetesServiceConnection"
                    kubernetesServiceConnection: "pixelated-aks-connection"
                    namespace: "$(KUBE_NAMESPACE)"
                    manifests: |
                      k8s/azure/staging/deployment.yaml
                      k8s/azure/staging/configmap.yaml
                      k8s/azure/staging/ingress.yaml

                - task: Kubernetes@1
                  displayName: "Update Image Tag"
                  inputs:
                    connectionType: "Kubernetes Service Connection"
                    kubernetesServiceEndpoint: "pixelated-aks-connection"
                    namespace: "$(KUBE_NAMESPACE)"
                    command: "set"
                    arguments: "image deployment/pixelated pixelated=$(DOCKER_REGISTRY)/$(IMAGE_REPOSITORY):$(Build.BuildNumber)"

                - task: Kubernetes@1
                  displayName: "Check Rollout Status"
                  inputs:
                    connectionType: "Kubernetes Service Connection"
                    kubernetesServiceEndpoint: "pixelated-aks-connection"
                    namespace: "$(KUBE_NAMESPACE)"
                    command: "rollout"
                    arguments: "status deployment/pixelated --timeout=300s"

                - task: Bash@3
                  displayName: "Create Sentry Release"
                  condition: succeededOrFailed()
                  inputs:
                    targetType: "inline"
                    script: |
                      # Don't use set -e so we can handle errors gracefully
                      set -uo pipefail

                      # Install Sentry CLI if not available
                      if ! command -v sentry-cli &> /dev/null; then
                        curl -sL https://sentry.io/get-cli/ | bash || {
                          echo "‚ö†Ô∏è Could not install sentry-cli, skipping release creation"
                          exit 0
                        }
                      fi

                      # Export Sentry configuration
                      export SENTRY_AUTH_TOKEN="$(SENTRY_AUTH_TOKEN)"
                      export SENTRY_ORG="$(SENTRY_ORG)"
                      export SENTRY_PROJECT="$(SENTRY_PROJECT)"

                      RELEASE_VERSION="${SENTRY_RELEASE:-$(Build.SourceVersion)}"

                      # Debug: Show configuration (without exposing token)
                      echo "üìã Sentry Configuration:"
                      echo "   Organization: $SENTRY_ORG"
                      echo "   Project: $SENTRY_PROJECT"
                      echo "   Release: $RELEASE_VERSION"
                      echo "   Auth Token: ${SENTRY_AUTH_TOKEN:0:10}... (truncated)"

                      # Check if variables are set
                      if [[ -z "$SENTRY_ORG" ]]; then
                        echo "‚ùå SENTRY_ORG is not set. Add it to Azure DevOps variable group."
                        exit 0
                      fi
                      if [[ -z "$SENTRY_PROJECT" ]]; then
                        echo "‚ùå SENTRY_PROJECT is not set. Add it to Azure DevOps variable group."
                        echo "   Expected value: pixel-astro"
                        exit 0
                      fi
                      if [[ -z "$SENTRY_AUTH_TOKEN" ]]; then
                        echo "‚ùå SENTRY_AUTH_TOKEN is not set. Add it to Azure DevOps variable group."
                        exit 0
                      fi

                      # Verify token has access to the organization
                      echo ""
                      echo "üîç Verifying Sentry access..."
                      if ! sentry-cli info --log-level=info 2>&1; then
                        echo "‚ùå Failed to verify Sentry authentication. Check that:"
                        echo "   1. SENTRY_AUTH_TOKEN is set correctly in Azure DevOps"
                        echo "   2. The token has 'org:read' and 'project:releases' scopes"
                        echo "   3. Organization slug '$SENTRY_ORG' is correct"
                        echo ""
                        echo "‚ö†Ô∏è Skipping release creation due to authentication issues"
                        exit 0
                      fi

                      # List available projects to help debug
                      echo ""
                      echo "üì¶ Available projects in organization '$SENTRY_ORG':"
                      AVAILABLE_PROJECTS=$(sentry-cli projects list 2>&1) || true
                      echo "$AVAILABLE_PROJECTS"

                      # Check if the configured project exists
                      if ! echo "$AVAILABLE_PROJECTS" | grep -q "$SENTRY_PROJECT"; then
                        echo ""
                        echo "‚ùå Project '$SENTRY_PROJECT' not found in organization '$SENTRY_ORG'"
                        echo ""
                        echo "   Possible fixes:"
                        echo "   1. Update SENTRY_PROJECT in Azure DevOps to match one of the projects above"
                        echo "   2. Create project '$SENTRY_PROJECT' in Sentry: https://sentry.io/settings/$SENTRY_ORG/projects/"
                        echo "   3. Ensure the auth token has access to the project"
                        echo ""
                        echo "‚ö†Ô∏è Skipping release creation due to project mismatch"
                        exit 0
                      fi

                      # Create Sentry release
                      echo ""
                      echo "üöÄ Creating Sentry release..."
                      if sentry-cli releases new "$RELEASE_VERSION" 2>&1; then
                        echo "‚úÖ Release created: $RELEASE_VERSION"
                      else
                        echo "‚ö†Ô∏è Release may already exist"
                      fi

                      # Set commits
                      if sentry-cli releases set-commits "$RELEASE_VERSION" --auto 2>&1; then
                        echo "‚úÖ Commits associated with release"
                      else
                        echo "‚ö†Ô∏è Could not set commits (may need GitHub integration)"
                      fi

                      # Finalize release
                      if sentry-cli releases finalize "$RELEASE_VERSION" 2>&1; then
                        echo "‚úÖ Release finalized"
                      else
                        echo "‚ö†Ô∏è Could not finalize release"
                      fi

                      # Create deploy
                      if sentry-cli releases deploys "$RELEASE_VERSION" new -e staging 2>&1; then
                        echo "‚úÖ Deploy recorded for staging environment"
                      else
                        echo "‚ö†Ô∏è Could not create deploy record"
                      fi

                      echo ""
                      echo "‚úÖ Sentry release process completed for $RELEASE_VERSION"
                  env:
                    SENTRY_AUTH_TOKEN: $(SENTRY_AUTH_TOKEN)

  - stage: DeployProduction
    displayName: "Deploy to Production (AKS)"
    dependsOn: DeployStaging
    condition: and(succeeded(), eq(variables['Build.SourceBranch'], 'refs/heads/master'))
    jobs:
      - deployment: DeployToProduction
        displayName: "Deploy to Production"
        pool:
          vmImage: "ubuntu-latest"
        environment: "production"
        strategy:
          runOnce:
            deploy:
              steps:
                - checkout: self

                - task: AzureCLI@2
                  displayName: "Configure AKS Credentials"
                  inputs:
                    azureSubscription: "$(AZURE_SUBSCRIPTION)"
                    scriptType: "bash"
                    scriptLocation: "inlineScript"
                    inlineScript: |
                      az aks get-credentials --resource-group $(AZURE_RESOURCE_GROUP) --name $(AKS_CLUSTER_NAME)

                - task: Bash@3
                  displayName: "Apply Production Secrets (suppress annotation warning)"
                  inputs:
                    targetType: "inline"
                    script: |
                      set -euo pipefail

                      # Apply secrets - kubectl will automatically patch the annotation
                      # Filter out the warning message to keep logs clean
                      kubectl apply -f k8s/azure/production/secrets.yaml --namespace $(KUBE_NAMESPACE_PROD) 2>&1 | \
                        grep -v "missing the kubectl.kubernetes.io/last-applied-configuration annotation" || true

                      echo "‚úÖ Secrets applied (annotation will be auto-patched by kubectl)"

                - task: KubernetesManifest@1
                  displayName: "Deploy to Production"
                  inputs:
                    action: "deploy"
                    connectionType: "kubernetesServiceConnection"
                    kubernetesServiceConnection: "pixelated-aks-connection"
                    namespace: "$(KUBE_NAMESPACE_PROD)"
                    manifests: |
                      k8s/azure/production/deployment.yaml
                      k8s/azure/production/configmap.yaml
                      k8s/azure/production/ingress.yaml

                - task: Kubernetes@1
                  displayName: "Update Production Image Tag"
                  inputs:
                    connectionType: "Kubernetes Service Connection"
                    kubernetesServiceEndpoint: "pixelated-aks-connection"
                    namespace: "$(KUBE_NAMESPACE_PROD)"
                    command: "set"
                    arguments: "image deployment/pixelated pixelated=$(DOCKER_REGISTRY)/$(IMAGE_REPOSITORY):$(Build.BuildNumber)"

                - task: Kubernetes@1
                  displayName: "Check Production Rollout Status"
                  inputs:
                    connectionType: "Kubernetes Service Connection"
                    kubernetesServiceEndpoint: "pixelated-aks-connection"
                    namespace: "$(KUBE_NAMESPACE_PROD)"
                    command: "rollout"
                    arguments: "status deployment/pixelated --timeout=300s"

                - task: Bash@3
                  displayName: "Create Sentry Production Release"
                  condition: succeededOrFailed()
                  inputs:
                    targetType: "inline"
                    script: |
                      # Don't use set -e so we can handle errors gracefully
                      set -uo pipefail

                      if ! command -v sentry-cli &> /dev/null; then
                        curl -sL https://sentry.io/get-cli/ | bash || {
                          echo "‚ö†Ô∏è Could not install sentry-cli, skipping release creation"
                          exit 0
                        }
                      fi

                      export SENTRY_AUTH_TOKEN="$(SENTRY_AUTH_TOKEN)"
                      export SENTRY_ORG="$(SENTRY_ORG)"
                      export SENTRY_PROJECT="$(SENTRY_PROJECT)"

                      RELEASE_VERSION="${SENTRY_RELEASE:-$(Build.SourceVersion)}"

                      # Debug: Show configuration (without exposing token)
                      echo "üìã Sentry Configuration (Production):"
                      echo "   Organization: $SENTRY_ORG"
                      echo "   Project: $SENTRY_PROJECT"
                      echo "   Release: $RELEASE_VERSION"
                      echo "   Auth Token: ${SENTRY_AUTH_TOKEN:0:10}... (truncated)"

                      # Verify token has access
                      echo ""
                      echo "üîç Verifying Sentry access..."
                      if ! sentry-cli info --log-level=info 2>&1; then
                        echo "‚ùå Failed to verify Sentry authentication"
                        echo "‚ö†Ô∏è Skipping release creation due to authentication issues"
                        exit 0
                      fi

                      # List available projects
                      echo ""
                      echo "üì¶ Available projects in organization '$SENTRY_ORG':"
                      sentry-cli projects list 2>&1 || echo "   (Could not list projects)"

                      # Create Sentry release
                      echo ""
                      echo "üöÄ Creating Sentry production release..."
                      if sentry-cli releases new "$RELEASE_VERSION" 2>&1; then
                        echo "‚úÖ Release created: $RELEASE_VERSION"
                      else
                        echo "‚ö†Ô∏è Release may already exist or project '$SENTRY_PROJECT' not found"
                      fi

                      if sentry-cli releases set-commits "$RELEASE_VERSION" --auto 2>&1; then
                        echo "‚úÖ Commits associated with release"
                      else
                        echo "‚ö†Ô∏è Could not set commits"
                      fi

                      if sentry-cli releases finalize "$RELEASE_VERSION" 2>&1; then
                        echo "‚úÖ Release finalized"
                      else
                        echo "‚ö†Ô∏è Could not finalize release"
                      fi

                      if sentry-cli releases deploys "$RELEASE_VERSION" new -e production 2>&1; then
                        echo "‚úÖ Deploy recorded for production environment"
                      else
                        echo "‚ö†Ô∏è Could not create deploy record"
                      fi

                      echo ""
                      echo "‚úÖ Sentry release process completed for $RELEASE_VERSION (production)"
                  env:
                    SENTRY_AUTH_TOKEN: $(SENTRY_AUTH_TOKEN)

  - stage: HealthCheck
    displayName: "Health Check"
    dependsOn: DeployStaging
    condition: succeeded()
    jobs:
      - job: HealthCheckStaging
        displayName: "Staging Health Check"
        pool:
          vmImage: "ubuntu-latest"
        steps:
          - task: AzureCLI@2
            displayName: "Check Deployment Health"
            inputs:
              azureSubscription: "$(AZURE_SUBSCRIPTION)"
              scriptType: "bash"
              scriptLocation: "inlineScript"
              inlineScript: |
                az aks get-credentials --resource-group $(AZURE_RESOURCE_GROUP) --name $(AKS_CLUSTER_NAME)

                # Check deployment status
                kubectl get deployment -n $(KUBE_NAMESPACE)
                kubectl get pods -n $(KUBE_NAMESPACE)

                # Wait for pods to be ready
                kubectl wait --for=condition=available --timeout=300s deployment/pixelated -n $(KUBE_NAMESPACE)

                echo "‚úÖ Deployment health check passed - all pods are ready"

  - stage: PerformanceTest
    displayName: "Performance Testing"
    dependsOn: DeployStaging
    condition: succeeded()
    jobs:
      - job: PerformanceTests
        displayName: "Run Performance Tests"
        pool:
          vmImage: "ubuntu-latest"
        steps:
          - task: NodeTool@0
            inputs:
              versionSpec: "$(NODE_VERSION)"
            displayName: "Install Node.js"

          - script: |
              corepack enable pnpm
              pnpm --version
            displayName: "Setup pnpm"

          - task: Cache@2
            inputs:
              key: 'pnpm | "$(Agent.OS)" | pnpm-lock.yaml'
              restoreKeys: |
                pnpm | "$(Agent.OS)"
              path: ~/.local/share/pnpm/store
            displayName: "Cache pnpm store"

          - script: |
              corepack enable pnpm
              pnpm install --frozen-lockfile
              pnpm run performance:test
            displayName: "Run Performance Tests"
            env:
              BASE_URL: $(STAGING_URL)

  - stage: E2ETest
    displayName: "E2E Testing"
    dependsOn: DeployStaging
    condition: succeeded()
    jobs:
      - job: E2ETests
        displayName: "Run E2E Tests"
        pool:
          vmImage: "ubuntu-latest"
        steps:
          - task: NodeTool@0
            inputs:
              versionSpec: "$(NODE_VERSION)"
            displayName: "Install Node.js"

          - script: |
              corepack enable pnpm
              pnpm install --frozen-lockfile
              pnpm run e2e:smoke
            displayName: "Run E2E Smoke Tests"
            env:
              BASE_URL: $(STAGING_URL)

          - publish: test-results/
            artifact: e2e-test-results
            displayName: "Publish Test Results"
            condition: always()

  # ============================================
  # OVH AI Training Stage (Manual Trigger)
  # ============================================
  - stage: OVHAITraining
    displayName: "OVH AI Training"
    # Manual trigger only - run with: az pipelines run --parameters TRIGGER_AI_TRAINING=true
    condition: and(succeeded(), eq(variables['TRIGGER_AI_TRAINING'], 'true'))
    jobs:
      - job: PrepareTrainingData
        displayName: "Prepare Training Data"
        pool:
          vmImage: "ubuntu-latest"
        steps:
          - checkout: self

          - task: Bash@3
            displayName: "Install ovhai CLI"
            inputs:
              targetType: "inline"
              script: |
                curl -sSL https://cli.us-east-va.ai.cloud.ovh.us/install.sh | bash
                export PATH="$HOME/bin:$PATH"
                ovhai --version || echo "CLI installed"

          - task: Bash@3
            displayName: "Sync Training Data to OVH"
            inputs:
              targetType: "inline"
              script: |
                export PATH="$HOME/bin:$PATH"

                # Authenticate using token
                ovhai login --token "$OVH_AI_TOKEN" || {
                  echo "‚ö†Ô∏è Token auth failed, manual login required"
                  exit 0
                }

                # Sync datasets
                cd ai/ovh
                ./sync-datasets.sh upload || echo "‚ö†Ô∏è Dataset sync may require manual intervention"
            env:
              OVH_AI_TOKEN: $(OVH_AI_TOKEN)

      - job: LaunchTrainingJob
        displayName: "Launch Training Job"
        dependsOn: PrepareTrainingData
        condition: succeeded()
        pool:
          vmImage: "ubuntu-latest"
        variables:
          OVH_GPU_MODEL: "L40S"
          OVH_MAX_HOURS: "12"
        steps:
          - checkout: self

          - task: Bash@3
            displayName: "Install ovhai CLI"
            inputs:
              targetType: "inline"
              script: |
                curl -sSL https://cli.us-east-va.ai.cloud.ovh.us/install.sh | bash
                export PATH="$HOME/bin:$PATH"

          - task: Bash@3
            displayName: "Build and Push Training Image"
            inputs:
              targetType: "inline"
              script: |
                export PATH="$HOME/bin:$PATH"

                # Authenticate
                ovhai login --token "$OVH_AI_TOKEN" || exit 1

                # Login to registry
                ovhai registry login

                # Build image
                cd ai
                docker build -f ovh/Dockerfile.training -t pixelated-training:$(Build.BuildNumber) .

                # Get registry URL and push
                REGISTRY=$(ovhai registry list --json | jq -r '.[0].url' || echo "")
                if [ -n "$REGISTRY" ]; then
                  docker tag pixelated-training:$(Build.BuildNumber) "$REGISTRY/pixelated-training:$(Build.BuildNumber)"
                  docker tag pixelated-training:$(Build.BuildNumber) "$REGISTRY/pixelated-training:latest"
                  docker push "$REGISTRY/pixelated-training:$(Build.BuildNumber)"
                  docker push "$REGISTRY/pixelated-training:latest"
                  echo "##vso[task.setvariable variable=OVH_IMAGE]$REGISTRY/pixelated-training:$(Build.BuildNumber)"
                else
                  echo "##vso[task.logissue type=warning]No OVH registry found"
                fi
            env:
              OVH_AI_TOKEN: $(OVH_AI_TOKEN)

          - task: Bash@3
            displayName: "Launch OVH Training Job"
            inputs:
              targetType: "inline"
              script: |
                set -euo pipefail
                export PATH="$HOME/bin:$PATH"

                # Authenticate
                ovhai login --token "$OVH_AI_TOKEN" || exit 1

                JOB_NAME="pixelated-training-$(Build.BuildNumber)"

                echo "üöÄ Launching training job: $JOB_NAME"
                echo "   GPU: $(OVH_GPU_MODEL)"
                echo "   Max hours: $(OVH_MAX_HOURS)"

                # Launch job
                ovhai job run \
                  --name "$JOB_NAME" \
                  --gpu 1 \
                  --gpu-model "$(OVH_GPU_MODEL)" \
                  --cpu 8 \
                  --memory 64Gi \
                  --volume "pixelated-training-data@US-EAST-VA:/data:ro" \
                  --volume "pixelated-checkpoints@US-EAST-VA:/checkpoints:rw" \
                  --env DATA_DIR=/data \
                  --env CHECKPOINT_DIR=/checkpoints \
                  --env CONFIG_PATH=/app/config/moe_training_config.json \
                  --env MAX_TRAINING_HOURS="$(OVH_MAX_HOURS)" \
                  --env WANDB_API_KEY="$WANDB_API_KEY" \
                  "$OVH_IMAGE" \
                  -- python /app/train_ovh.py --config /app/config/moe_training_config.json --max-hours "$(OVH_MAX_HOURS)" || {
                    echo "##vso[task.logissue type=warning]Failed to launch training job"
                    exit 1
                  }

                echo "‚úÖ Training job launched: $JOB_NAME"
                echo "üìä Monitor with: ovhai job logs $JOB_NAME -f"
            env:
              OVH_AI_TOKEN: $(OVH_AI_TOKEN)
              WANDB_API_KEY: $(WANDB_API_KEY)
              OVH_IMAGE: $(OVH_IMAGE)

  # ============================================
  # OVH Ollama Image Build Stage (Manual Trigger)
  # ============================================
  - stage: OVHOllamaBuild
    displayName: "OVH Ollama Image Build"
    # Manual trigger only - run with: az pipelines run --parameters TRIGGER_OVH_OLLAMA_BUILD=true
    condition: eq(variables['TRIGGER_OVH_OLLAMA_BUILD'], 'true')
    jobs:
      - job: BuildOllamaImage
        displayName: "Build and Push Ollama Image"
        pool:
          vmImage: "ubuntu-latest"
        steps:
          - checkout: self

          - task: Bash@3
            displayName: "Install ovhai CLI"
            inputs:
              targetType: "inline"
              script: |
                curl -sSL https://cli.us-east-va.ai.cloud.ovh.us/install.sh | bash
                export PATH="$HOME/bin:$PATH"
                ovhai --version || echo "CLI installed"

          - task: Bash@3
            displayName: "Build and Push Ollama Image"
            inputs:
              targetType: "inline"
              script: |
                export PATH="$HOME/bin:$PATH"

                # Authenticate
                ovhai login --token "$OVH_AI_TOKEN" || exit 1

                # Login to registry
                ovhai registry login

                # Build image
                cd ai
                docker build -f ovh/Dockerfile.ollama -t pixelated-ollama:$(Build.BuildNumber) .

                # Get registry URL and push
                REGISTRY=$(ovhai registry list | tail -n +2 | awk '{print $NF}' || echo "")
                if [ -n "$REGISTRY" ]; then
                  docker tag pixelated-ollama:$(Build.BuildNumber) "$REGISTRY/pixelated-ollama:$(Build.BuildNumber)"
                  docker tag pixelated-ollama:$(Build.BuildNumber) "$REGISTRY/pixelated-ollama:latest"
                  docker push "$REGISTRY/pixelated-ollama:$(Build.BuildNumber)"
                  docker push "$REGISTRY/pixelated-ollama:latest"
                  echo "##vso[task.setvariable variable=OVH_OLLAMA_IMAGE]$REGISTRY/pixelated-ollama:$(Build.BuildNumber)"
                  echo "‚úÖ Image pushed: $REGISTRY/pixelated-ollama:latest"
                else
                  echo "##vso[task.logissue type=warning]No OVH registry found"
                  exit 1
                fi
            env:
              OVH_AI_TOKEN: $(OVH_AI_TOKEN)

  - stage: SchedulePosts
    displayName: "Schedule Blog Posts"
    condition: or(eq(variables['Build.Reason'], 'Schedule'), eq(variables['Build.Reason'], 'Manual'))
    jobs:
      - job: ScheduleBlogPosts
        displayName: "Schedule and Publish Blog Posts"
        pool:
          vmImage: "ubuntu-latest"
        steps:
          - checkout: self
            persistCredentials: "true"

          - task: NodeTool@0
            inputs:
              versionSpec: "$(NODE_VERSION)"
            displayName: "Install Node.js"

          - script: |
              corepack enable pnpm
              pnpm --version
            displayName: "Enable pnpm"

          - script: |
              pnpm store prune || true
              pnpm install --no-frozen-lockfile
            displayName: "Install Dependencies"

          - task: Bash@3
            displayName: "Configure Git"
            inputs:
              targetType: "inline"
              script: |
                git config --global user.name "Azure DevOps"
                git config --global user.email "azure-pipelines@devops.com"

          - script: |
              pnpm run schedule-posts
            displayName: "Run Post Scheduler"
            env:
              GITHUB_ACTIONS: "$(GITHUB_ACTIONS)"
              NODE_ENV: "$(NODE_ENV)"
              SYSTEM_ACCESSTOKEN: $(System.AccessToken)
